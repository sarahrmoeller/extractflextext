{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET # parses XML files\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data from flextext XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders/delimiters\n",
    "TEMP = '@@@'\n",
    "\n",
    "# text-level flextext XML attributes\n",
    "TITLE_TYPE = 'title'\n",
    "COMMENT_TYPE = 'comment'\n",
    "# flextext XML attributes for languages/scripts used in title or translations\n",
    "ENGLISH = 'en'\n",
    "INDONESIAN = 'id'\n",
    "\n",
    "# IGT tier-level flextext XML attributes\n",
    "TXT = 'txt' # surface morph/segment AND transcribed text\n",
    "CAN_MORPHEME = 'cf' # canonical (underlying) morpheme\n",
    "GLOSS = 'gls' # morpheme gloss AND sentence free translation\n",
    "M_POS = 'msa' # morpheme-level pos, what category affix attaches to\n",
    "WORD_POS = 'pos' # word-level pos\n",
    "PUNCT = 'punct' # punctuation\n",
    "\n",
    "# morpheme types (flextext XML attributes)\n",
    "MWE = 'phrase' # multiword expression\n",
    "PREFIX = 'prefix'\n",
    "SUFFIX = 'suffix'\n",
    "CIRCUMFIX = 'circumfix'\n",
    "PROCLITIC = 'proclitic'\n",
    "ENCLITICS = ['enclitic', 'clitic'] # NOTE: clitic functions as enclitic in some FLEx databases (e.g. lmk)\n",
    "INFIXES = ['infix', 'infixing interfix']\n",
    "STEMS = ['stem', 'bound stem', 'bound root', 'bound root A', 'root', 'particle']    \n",
    "\n",
    "# Segment boundaries are  uniquely marked in FLEx, add yours here\n",
    "# NOTE: FLEx databases handle circumfixes differently\n",
    "# NOTE: these symbols will need to be removed before re-importing to FLEx\n",
    "CIRCUM_PRE = '>'\n",
    "CIRCUM_POST = '<'\n",
    "CIRCUM_HOLE = '<>'\n",
    "CLITIC = '='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitleComment(xmlsection):\n",
    "    '''find title and comment if in this section\n",
    "    some documents have both and english and native language titles\n",
    "    these checks assure that the both will always be used if found separated by //\n",
    "    if only one of them is found then it is used\n",
    "    if none are found return NO TITLE FOUND'''\n",
    "    \n",
    "    title = \"NO TITLE FOUND\" \n",
    "    eng_title = TEMP\n",
    "    non_eng_title = TEMP\n",
    "    comment = \"No comment\"\n",
    "    \n",
    "    for item_lin in xmlsection.iter('item'):\n",
    "        if item_lin.get('type') == TITLE_TYPE and item_lin.get('lang') == ENGLISH:\n",
    "            eng_title = item_lin.text\n",
    "        if item_lin.get('type') == TITLE_TYPE and item_lin.get('lang') != ENGLISH:\n",
    "            non_eng_title = item_lin.text\n",
    "        if item_lin.get('type') == COMMENT_TYPE and item_lin.get('lang') == ENGLISH:\n",
    "            comment = item_lin.text\n",
    "    # check languages of title and add either or both\n",
    "    if eng_title != TEMP and non_eng_title == TEMP:\n",
    "        title = eng_title \n",
    "    elif eng_title == TEMP and non_eng_title != TEMP:\n",
    "        title = non_eng_title\n",
    "    elif eng_title != TEMP and non_eng_title != TEMP:\n",
    "        title = eng_title + ' // ' + non_eng_title \n",
    "        \n",
    "    return title, comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These cleaning functions handle pecularities or non-conventional annotation of IGT\n",
    "\n",
    "def cleanWord(IGTstring):\n",
    "    '''formats word to reduce confusion'''\n",
    "    \n",
    "    #TODO: reverse before reimporting to FLEx\n",
    "    \n",
    "    IGTstring = str(IGTstring)\n",
    "    \n",
    "    # TODO?: phrasal lexemes separated by double tilde\n",
    "    #IGTstring = IGTstring.strip().replace(' ', '~~')\n",
    "    # use tilde for hyphenated words\n",
    "    IGTstring = IGTstring.replace('-', '~')\n",
    "    \n",
    "    return IGTstring.strip().lower()\n",
    "    \n",
    "    \n",
    "def cleanMorph(IGTstring):\n",
    "    '''remove unexpected symbols in surface morphs and canonical morphemes\n",
    "    (includes infixes and circumfix halves)'''\n",
    "    \n",
    "    #TODO: reverse before reimporting to FLEx\n",
    "    \n",
    "    # separate multiple words in morpheme string with period\n",
    "    IGTstring = IGTstring.replace(' ', '.')\n",
    "    \n",
    "    IGTstring = IGTstring.lower()\n",
    "    \n",
    "    # make null morpheme symbol consistent across databases, avoid encoding bugs\n",
    "    IGTstring = IGTstring.replace('Ø','NULL').replace('∅', 'NULL').replace('zero', 'NULL')\n",
    "    # add your null morpheme symbol here\n",
    "    IGTstring = IGTstring.replace('*0','NULL') # lez\n",
    "    \n",
    "    # NOTE: add here any pre-processing specific to a database\n",
    "    #IGTstring = IGTstring.replace('*', '') # NTU\n",
    "    \n",
    "    return IGTstring.strip()\n",
    "\n",
    "\n",
    "def cleanGloss(IGTstring, morpheme_type):\n",
    "    '''preprocess morpheme glosses\n",
    "    Follow Leipzig glossing rules where possible'''\n",
    "    \n",
    "    # separate multiple words in glosses with period, per linguistic convention\n",
    "    IGTstring = IGTstring.replace('-','.').replace(' ', '.')\n",
    "    \n",
    "    # make affix glosses all caps, per linguistic convention\n",
    "    if morpheme_type not in STEMS:\n",
    "        IGTstring = IGTstring.upper()\n",
    "    \n",
    "    return IGTstring.strip()\n",
    "\n",
    "\n",
    "def cleanPOS(IGTstring):\n",
    "    '''preprocess morpheme-level POS and word-level POS'''\n",
    "    \n",
    "    #TODO: reverse before returning to FLEx\n",
    "    \n",
    "    # separate multiple tags with period, per linguistic convention\n",
    "    IGTstring = IGTstring.replace(' ', '')\n",
    "    # remove FLEx-inserted hyphens, to reduce confusion w morpheme delimiter\n",
    "    #TODO: reverse before returning to FLEx\n",
    "    IGTstring = IGTstring.replace('pro-form', 'proform').replace('Nom-1','Nom1')\n",
    "    \n",
    "    # NOTE: add here any pre-processing specific to a database\n",
    "    IGTstring = IGTstring.replace('N (kx cl)', 'N(kx.cl)') ## Natugu [ntu] morpheme pos\n",
    "    \n",
    "    return IGTstring.strip()   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfixedStem(wordtxt, morphitem, infix):\n",
    "    '''infixed stems need special processing,\n",
    "    especially for non-neural models that require glosses for every segment'''\n",
    "    \n",
    "    pre_temp_morph = [TEMP, TEMP, TEMP, TEMP]\n",
    "    post_temp_morph = [TEMP, TEMP, TEMP, TEMP]\n",
    "    \n",
    "    infix = infix[0][1:-1] # remove dashes surrounding infixes\n",
    "    stemhalves = wordtxt.split(infix) # treat strings surrounding infixes as stems\n",
    "    \n",
    "    # get other tiers\n",
    "    for item in morphitem.iter('item'):\n",
    "        if item.get('type') != None or item.text != '' or item.text != '<NotSure>' or item.text != ' ':\n",
    "            # get surface morph, treat same as stem halves\n",
    "            if (item.get('type') == TXT):\n",
    "                pre_temp_morph[0] = cleanGloss(stemhalves[0])\n",
    "                post_temp_morph[0] = cleanGloss(stemhalves[1])\n",
    "            # canonical morpheme, will be nothing for first half if infixed\n",
    "            elif(item.get('type') == CAN_MORPHEME):\n",
    "                pre_temp_morph[1] = cleanMorph(item.text)\n",
    "                post_temp_morph[1] = cleanMorph(item.text)\n",
    "            # gloss, same for both\n",
    "            elif(item.get('type') == GLOSS):\n",
    "                # separate multi-word glosses with \".\"\n",
    "                pre_temp_morph[2] = cleanGloss(item.text)\n",
    "                post_temp_morph[2] = cleanGloss(item.text)\n",
    "            # morpheme pos\n",
    "            elif(item.get('type') == M_POS):\n",
    "                pre_temp_morph[3] = cleanPOS(item.text)\n",
    "                post_temp_morph[3] = cleanPOS(item.text)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return pre_temp_morph, post_temp_morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMorpheme(morphitem, morphemetype):\n",
    "    '''OUTPUT for each morpheme segment: [morph, morpheme, gloss, mpos]\n",
    "    To add more items to this array of info about morpheme segments:\n",
    "    1st. Add another holding place in the morph_info array; give index for that info piece.\n",
    "    2nd. Add elif statement for new tier using the attribute you want, e.g. 'morpheme type'.\n",
    "        If necessary, create special delimiter and write \"cleaning\" function.\n",
    "    3rd. Check that that morph_info array matches entries in temp_morph\n",
    "        and does not mess up punctuation processing.'''\n",
    "    \n",
    "    # temporary array for morpheme information\n",
    "    morph_info = [TEMP, TEMP, TEMP, TEMP]\n",
    "    # indexes for types of information to be in morph_info\n",
    "    MORPH_IDX = 0\n",
    "    MORPHEME_IDX = 1\n",
    "    GLOSS_IDX = 2\n",
    "    M_POS_IDX = 3\n",
    "    \n",
    "    # make uniform label for all stem-like morphemes\n",
    "    # assume missing morpheme type attribute is a stem\n",
    "    if morphemetype == None or morphemetype in STEMS:\n",
    "        morphemetype = 'stem'        \n",
    "    \n",
    "    # catch \"new\" morpheme types in current database\n",
    "    if (morphemetype not in STEMS and morphemetype not in INFIXES\n",
    "        and morphemetype != PROCLITIC and morphemetype not in ENCLITICS\n",
    "        and morphemetype != PREFIX and morphemetype != SUFFIX\n",
    "        and morphemetype != MWE and morphemetype != CIRCUMFIX):\n",
    "            print(\"\\nThis morpheme type XML attribute is not handled yet in getMorpheme(): \" + morphemetype)\n",
    "    \n",
    "    # extract information about morpheme from IGT tiers\n",
    "    for item in morphitem.iter('item'):\n",
    "        if item.text != None:\n",
    "            # surface morph (txt)\n",
    "            if (item.get('type') == TXT):\n",
    "                if morphemetype in ENCLITICS:\n",
    "                    morph_info[MORPH_IDX] = CLITIC + cleanMorph(item.text)\n",
    "                elif morphemetype == PROCLITIC:\n",
    "                    morph_info[MORPH_IDX] = cleanMorph(item.text) + CLITIC\n",
    "                else:\n",
    "                    morph_info[MORPH_IDX] = cleanMorph(item.text)\n",
    "            # TIER: canonical morpheme (cf)\n",
    "            elif(item.get('type') == CAN_MORPHEME):\n",
    "                # TODO: do not assume only 1 circumfix per word\n",
    "                if morphemetype == CIRCUMFIX: \n",
    "                    # if first half of circumfix is word-initial, treat as prefix\n",
    "                    if numaffix == 1:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text) + CIRCUM_PRE\n",
    "                    # if first half of circumfix is not word-initial, treat as infix\n",
    "                    else:\n",
    "                        morph_info[MORPHEME_IDX] = CIRCUM_POST + cleanMorph(item.text) + CIRCUM_PRE\n",
    "                # treat halves circumfix as pre/suffix, treat circumfixed stem as stem\n",
    "                elif '-...-' in item.text:\n",
    "                    if morphemetype in STEMS or morphemetype == MWE:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text).replace('-...-', '')\n",
    "                    elif morphemetype == PREFIX:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text).replace('-...-', CIRCUM_PRE)\n",
    "                    elif morphemetype == SUFFIX:\n",
    "                        morph_info[MORPHEME_IDX] = CIRCUM_POST + cleanMorph(item.text).replace('-...-', '')\n",
    "                # other canonical morpheme types\n",
    "                else:\n",
    "                    if morphemetype in ENCLITICS:\n",
    "                        morph_info[MORPHEME_IDX] = CLITIC + cleanMorph(item.text)\n",
    "                    elif morphemetype == PROCLITIC:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text) + CLITIC\n",
    "                    else:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text)\n",
    "            # TIER: gloss\n",
    "            elif (item.get('type') == GLOSS):\n",
    "                morph_info[GLOSS_IDX] = cleanGloss(item.text, morphemetype)\n",
    "            # TIER: morpheme pos\n",
    "            elif(item.get('type') == M_POS):\n",
    "                morph_info[M_POS_IDX] = cleanPOS(item.text)\n",
    "                \n",
    "    return morph_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_flextext(flextext_filename):\n",
    "    '''Takes FLExText XML any number of texts. \n",
    "    Extracts words. OUTPUT: \n",
    "    [[text_title, \n",
    "      text_comment, \n",
    "      [*words*\n",
    "        [*word*\n",
    "        [line#, word, wPOS, morph, morpheme, gloss, mpos, morphemetype]*morpheme*\n",
    "        ]]]]'''\n",
    "    \n",
    "    #TODO: change output format to JSON/dictionary\n",
    "    \n",
    "    \n",
    "    root = ET.parse(flextext_filename).getroot()\n",
    "    texts = []\n",
    "    total_lexemes = 0 # NOTE: MWE is 1 lexeme\n",
    "    \n",
    "    for text in root.iter('interlinear-text'):\n",
    "        title,comment = getTitleComment(text)\n",
    "        temp_text = [title, comment]\n",
    "        temp_words = []\n",
    "        \n",
    "        # ignoring paragraph breaks\n",
    "        for phrase in text.iter('phrase'):    \n",
    "            # FLEx \"segnum\" is ID for phrase/line/sentence\n",
    "            segnum = TEMP\n",
    "            if phrase.find('item').get('type') == 'segnum':\n",
    "                segnum = phrase.find('item').text\n",
    "            else:\n",
    "                segnum = 'NoLine#'\n",
    "\n",
    "            # \"words\" or MWE as tokenized by FLEx user or whitespace\n",
    "            for word in phrase.iter('word'):\n",
    "                wordtype = word.find('item').get('type')\n",
    "                wordstring = cleanWord(word.find('item').text)\n",
    "                \n",
    "                # ignore punctuation & digits\n",
    "                if (wordtype != PUNCT and not wordstring.isdigit()):\n",
    "                    temp_word = []\n",
    "                    affix_order = [] # to align infixes \n",
    "                    total_lexemes+=1\n",
    "        \n",
    "                    # get word POS\n",
    "                    temp_wpos = TEMP # word-level POS\n",
    "                    for word_item in word.iter('item'):\n",
    "                        if word_item.get('type') == WORD_POS:\n",
    "                            temp_wpos = cleanPOS(word_item.text)\n",
    "\n",
    "                    # get interlinear for word segments, if any\n",
    "                    if word.find('morphemes') != None:\n",
    "                        for morph in word.iter('morph'):\n",
    "                            temp_segment = [segnum, wordstring, temp_wpos]\n",
    "                            morphemetype = morph.get('type')\n",
    "                            \n",
    "                            # TODO: for non-neural models (need input/output alignment)\n",
    "                            # morpheme type will determine what part of string is infix\n",
    "                            #affix_order.append(morphemetype)\n",
    "                            # handle infixes\n",
    "                            #if len(affix_order) >= 2 and affix_order[-2] in infixes:\n",
    "                                # NOTE: FLEx seems to always put infix before its stem\n",
    "                                #preinfix, postinfix = getInfixedStem(str(wrd), morph, temp_word[-1])\n",
    "                                # insert first half of prefix for surface segmentation\n",
    "                                #infix_index = len(affix_order)-2\n",
    "                                #temp_word.insert(infix_index-1, preinfix)\n",
    "                                # add second half of infixed stem\n",
    "                                #temp_morph = postinfix\n",
    "                            #else:\n",
    "                            \n",
    "                            temp_morph = getMorpheme(morph, morphemetype)\n",
    "                            \n",
    "                            # Add generic gloss to unglossed proper nouns\n",
    "                            # check gloss index\n",
    "                            if temp_wpos == 'nprop' and morphemetype in STEMS:\n",
    "                                if temp_morph[2] == TEMP: \n",
    "                                    temp_morph[2] = 'proper_name'\n",
    "                            \n",
    "                            # add morpheme to list of word's segments\n",
    "                            temp_segment.extend(temp_morph)\n",
    "                            temp_segment.append(morphemetype)\n",
    "                            \n",
    "                            temp_word.append(temp_segment)\n",
    "                            \n",
    "                    else:\n",
    "                        temp_word = [[segnum, wordstring, temp_wpos, TEMP, TEMP, TEMP, TEMP, TEMP]]\n",
    "                        \n",
    "                    temp_words.append(temp_word)\n",
    "            \n",
    "            # TODO: get free translations\n",
    "            # TODO: handle as many languages if needed\n",
    "            #en_translation = TEMP\n",
    "            #id_translation = TEMP\n",
    "            #temp_phrase_gloss = [p_item for p_item in phrase.iter('item')]\n",
    "            # make sure the last item is indeed our phrase translation\n",
    "            #for tpg in temp_phrase_gloss:\n",
    "            #    if tpg.get('type') == gloss and tpg.get('lang') == ENGLISH:\n",
    "            #        en_translation = tpg.text\n",
    "            #    if tpg.get('type') == gloss and tpg.get('lang') == INDONESIAN:\n",
    "            #        id_translation = tpg.text\n",
    "            #append metadata the translation of the phrase to the end of the temp line\n",
    "            #temp_line.append(en_translation)\n",
    "            #temp_line.append(id_translation)\n",
    "            \n",
    "        # add words to text\n",
    "        temp_text.append(temp_words)\n",
    "        texts.append(temp_text)\n",
    "    \n",
    "    # corpus statistics\n",
    "    print(\"Total tokenized lexemes, ignoring punctuation and digits:\", total_lexemes, end='\\n\\n')\n",
    "    # sanity check first 10 words\n",
    "    print(texts[0][2][:18], end='\\n\\n')\n",
    "    \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glossed(wordlistofmorphemes):\n",
    "    '''checks for glosses, \n",
    "        assumes segmentation is complete'''\n",
    "    \n",
    "    glossed = True\n",
    "    for segment in wordlistofmorphemes:\n",
    "        if segment[5] == TEMP:\n",
    "            glossed = False\n",
    "            break # this line saves time \n",
    "    return glossed\n",
    "    \n",
    "    \n",
    "def annotated(wordlistofmorphemes):\n",
    "    '''skipping words that have not been annotated \n",
    "    (i.e. no <morphemes> tag in XML)'''\n",
    "    \n",
    "    annotated = True\n",
    "    if len(wordlistofmorphemes) == 1:\n",
    "        if wordlistofmorphemes[0][3] == TEMP:\n",
    "            annotated = False\n",
    "    return annotated\n",
    "\n",
    "\n",
    "def multiword(wordlistofmorphemes):\n",
    "    \n",
    "    mwe = False\n",
    "    if ' ' in wordlistofmorphemes[0][1]:\n",
    "        mwe = True\n",
    "    return mwe\n",
    "\n",
    "\n",
    "def quality_check(extractedtexts):\n",
    "    '''Write custom filter functions above, add calls here\n",
    "        Un/comment lines with filter calls as needed'''\n",
    "    \n",
    "    good = []\n",
    "    bad = []\n",
    "    good_cnt = 0\n",
    "    \n",
    "    for text in extractedtexts:\n",
    "        for word in text[-1]:\n",
    "            if glossed(word) and annotated(word) and not multiword(word):\n",
    "                good.append(word)\n",
    "                good_cnt+=1\n",
    "            else:\n",
    "                bad.append(word)\n",
    "    \n",
    "    print(\"Total segmented and glossed lexemes:\", good_cnt, end='\\n\\n')\n",
    "    return good,bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_alignment(a, b):\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(\"morph(emes) and gloss must be same amount in a word\")\n",
    "        \n",
    "\n",
    "def extract2file(extracted_data, purpose, outfilepath):\n",
    "    '''Writes two files: x and Y (data and annotations; input and output)\n",
    "    No text or line divisions, just list of tokens\n",
    "    Purpose variable determines what will be preserved in file,\n",
    "    possibilities match output types variables\n",
    "    gls = glossing only, seggls = segmentation+glossing, pos = (word) POS tagging'''\n",
    "    \n",
    "    input_data = []\n",
    "    output_data = []\n",
    "    \n",
    "    for word in extracted_data: \n",
    "        input_data.append(' '.join(word[0][1])) # text token with space delimiter\n",
    "            \n",
    "        # output types\n",
    "        wPOS_tag = word[0][2]\n",
    "        canonical_morphemes = []\n",
    "        surface_morphemes = []\n",
    "        morpheme_glosses = []\n",
    "        for morpheme in word:\n",
    "            canonical_morphemes.append(morpheme[3])\n",
    "            surface_morphemes.append(morpheme[4])\n",
    "            morpheme_glosses.append(morpheme[5])\n",
    "\n",
    "        # final output data\n",
    "        if purpose == '_pos':\n",
    "            output_data.append(wPOS_tag)\n",
    "        elif purpose == '_can_seg':\n",
    "            output_data.append(' '.join(canonical_morphemes))\n",
    "        elif purpose == '_surf_seg':\n",
    "            output_data.append(' '.join(surface_morphemes))\n",
    "        #TODO: purpose == 'can_seg_gls' ...allow for null morphemes\n",
    "        elif purpose == '_surf_seg_gls':\n",
    "            check_alignment(surface_morphemes, morpheme_glosses)\n",
    "            combined_seg_gls = []\n",
    "            for i, morpheme in enumerate(surface_morphemes):\n",
    "                combined_seg_gls.append(morpheme+'#'+morpheme_glosses[i])\n",
    "            output_data.append(' '.join(combined_seg_gls))\n",
    "\n",
    "    with open(outfilepath+purpose+'.input', 'w', encoding='utf8') as I, open(outfilepath+purpose+'.output', 'w', encoding='utf8') as O:\n",
    "        I.write('\\n'.join(input_data))\n",
    "        O.write('\\n'.join(output_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Run Code: Extract Surface Segmentation Data to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " lez-all_txts_2019\n",
      "Total tokenized lexemes, ignoring punctuation and digits: 13988\n",
      "\n",
      "[[['NoLine#', 'са', 'cardnum', 'са', 'са', 'one', 'cardnum', 'stem']], [['NoLine#', 'юкъуз', 'n', 'юкъ', 'юкъ', '@@@', 'n', 'stem'], ['NoLine#', 'юкъуз', 'n', '-у', '-ди', 'OBL', 'n:(OBLIQUEstem)', 'suffix'], ['NoLine#', 'юкъуз', 'n', '-з', '-з', 'DAT', 'n:(CASE)', 'suffix']], [['NoLine#', 'зун', 'pers', 'зун', 'зун', '1sg.abs', 'pers', 'stem']], [['NoLine#', 'хуьряй', 'n', 'хуьр', 'хуьр', 'village', 'n', 'stem'], ['NoLine#', 'хуьряй', 'n', '-ай', '-ай', 'INELAT', 'n:(OBLIQUEstem)', 'suffix']], [['NoLine#', 'кцӏариз', 'n', 'кцӏар', 'кцӏар', 'gusar', 'n', 'stem'], ['NoLine#', 'кцӏариз', 'n', '-и', '-ди', 'OBL', 'n:(OBLIQUEstem)', 'suffix'], ['NoLine#', 'кцӏариз', 'n', '-з', '-з', 'DAT', 'n:(CASE)', 'suffix']], [['NoLine#', 'хквезвай', 'Vnf', 'хкве', 'хквен', 'return', 'v', 'stem'], ['NoLine#', 'хквезвай', 'Vnf', '-зва', '-зва', 'IMPF', 'v:TAM1', 'suffix'], ['NoLine#', 'хквезвай', 'Vnf', '-й', '-й', 'PST', 'cop:(TAM)', 'suffix']], [['NoLine#', 'тир', 'v', 'тир', 'тир', 'was', 'v', 'stem']], [['NoLine#', 'рекъе', 'n', 'рекъ', 'рекъ', 'way,.road', 'n', 'stem'], ['NoLine#', 'рекъе', 'n', '-е', '-е', 'INESS', 'n:(OBLIQUEstem)', 'suffix']], [['NoLine#', 'са', 'cardnum', 'са', 'са', 'one', 'cardnum', 'stem']], [['NoLine#', 'стӏурви', 'n', 'стӏурви', 'стӏурви', 'from.Sudur', 'n', 'stem']], [['NoLine#', 'стхадал', 'n', 'стха', 'стха', 'brother', 'n', 'stem'], ['NoLine#', 'стхадал', 'n', '-да', '-е', 'INESS', 'n:(OBLIQUEstem)', 'suffix'], ['NoLine#', 'стхадал', 'n', '-л', '-л', 'SPSS', 'n:(CASE)', 'suffix']], [['NoLine#', 'гьалтна', 'Vf', 'гьалт', 'гьалтун', 'meet', 'v', 'stem'], ['NoLine#', 'гьалтна', 'Vf', '-на', '-на', 'AOR', 'v:TAM1', 'suffix']], [['NoLine#', 'таниш', 'v', 'таниш', 'таниш', 'acquainted', 'adj', 'stem']], [['NoLine#', 'хьана', 'Vnf', 'хьа', 'хьун', 'be', 'v', 'stem'], ['NoLine#', 'хьана', 'Vnf', '-на', '-на', 'AOC', 'v:TAM1', 'suffix']], [['NoLine#', 'жув', 'proform', 'жув', 'жув', 'us', 'proform', 'stem']], [['NoLine#', 'чирна', 'Vf', 'чир', 'чирин', 'know', 'v', 'stem'], ['NoLine#', 'чирна', 'Vf', '-на', '-на', 'AOR', 'v:TAM1', 'suffix']], [['NoLine#', 'гьадаз', 'pro', 'гьада', 'гьада', 'she', 'pro', 'stem'], ['NoLine#', 'гьадаз', 'pro', '-з', '-з', 'DAT', 'n:(CASE)', 'suffix']], [['NoLine#', 'адаз', 'pro', 'ада', 'ада', 'he', 'pro', 'stem'], ['NoLine#', 'адаз', 'pro', '-з', '-з', 'DAT', 'n:(CASE)', 'suffix']]]\n",
      "\n",
      "Total segmented and glossed lexemes: 13187\n",
      "\n",
      "[[['NoLine#', 'са', 'cardnum', 'са', 'са', 'one', 'cardnum', 'stem']], [['NoLine#', 'зун', 'pers', 'зун', 'зун', '1sg.abs', 'pers', 'stem']], [['NoLine#', 'хуьряй', 'n', 'хуьр', 'хуьр', 'village', 'n', 'stem'], ['NoLine#', 'хуьряй', 'n', '-ай', '-ай', 'INELAT', 'n:(OBLIQUEstem)', 'suffix']], [['NoLine#', 'кцӏариз', 'n', 'кцӏар', 'кцӏар', 'gusar', 'n', 'stem'], ['NoLine#', 'кцӏариз', 'n', '-и', '-ди', 'OBL', 'n:(OBLIQUEstem)', 'suffix'], ['NoLine#', 'кцӏариз', 'n', '-з', '-з', 'DAT', 'n:(CASE)', 'suffix']], [['NoLine#', 'хквезвай', 'Vnf', 'хкве', 'хквен', 'return', 'v', 'stem'], ['NoLine#', 'хквезвай', 'Vnf', '-зва', '-зва', 'IMPF', 'v:TAM1', 'suffix'], ['NoLine#', 'хквезвай', 'Vnf', '-й', '-й', 'PST', 'cop:(TAM)', 'suffix']], [['NoLine#', 'тир', 'v', 'тир', 'тир', 'was', 'v', 'stem']], [['NoLine#', 'рекъе', 'n', 'рекъ', 'рекъ', 'way,.road', 'n', 'stem'], ['NoLine#', 'рекъе', 'n', '-е', '-е', 'INESS', 'n:(OBLIQUEstem)', 'suffix']], [['NoLine#', 'са', 'cardnum', 'са', 'са', 'one', 'cardnum', 'stem']], [['NoLine#', 'стӏурви', 'n', 'стӏурви', 'стӏурви', 'from.Sudur', 'n', 'stem']], [['NoLine#', 'стхадал', 'n', 'стха', 'стха', 'brother', 'n', 'stem'], ['NoLine#', 'стхадал', 'n', '-да', '-е', 'INESS', 'n:(OBLIQUEstem)', 'suffix'], ['NoLine#', 'стхадал', 'n', '-л', '-л', 'SPSS', 'n:(CASE)', 'suffix']], [['NoLine#', 'гьалтна', 'Vf', 'гьалт', 'гьалтун', 'meet', 'v', 'stem'], ['NoLine#', 'гьалтна', 'Vf', '-на', '-на', 'AOR', 'v:TAM1', 'suffix']], [['NoLine#', 'таниш', 'v', 'таниш', 'таниш', 'acquainted', 'adj', 'stem']], [['NoLine#', 'хьана', 'Vnf', 'хьа', 'хьун', 'be', 'v', 'stem'], ['NoLine#', 'хьана', 'Vnf', '-на', '-на', 'AOC', 'v:TAM1', 'suffix']], [['NoLine#', 'жув', 'proform', 'жув', 'жув', 'us', 'proform', 'stem']], [['NoLine#', 'чирна', 'Vf', 'чир', 'чирин', 'know', 'v', 'stem'], ['NoLine#', 'чирна', 'Vf', '-на', '-на', 'AOR', 'v:TAM1', 'suffix']], [['NoLine#', 'гьадаз', 'pro', 'гьада', 'гьада', 'she', 'pro', 'stem'], ['NoLine#', 'гьадаз', 'pro', '-з', '-з', 'DAT', 'n:(CASE)', 'suffix']], [['NoLine#', 'адаз', 'pro', 'ада', 'ада', 'he', 'pro', 'stem'], ['NoLine#', 'адаз', 'pro', '-з', '-з', 'DAT', 'n:(CASE)', 'suffix']], [['NoLine#', 'хьи', 'coordconn', 'хьи', 'хьи', 'that', 'coordconn', 'stem']]]\n",
      "\n",
      " lez-all_txts_2022\n",
      "Total tokenized lexemes, ignoring punctuation and digits: 13991\n",
      "\n",
      "[[['NoLine#', 'са', 'cardnum', 'са', 'са', 'one', 'cardnum', 'stem']], [['NoLine#', 'юкъуз', 'n', 'юкъ', '@@@', '@@@', '@@@', None], ['NoLine#', 'юкъуз', 'n', '-у', '-ди', 'OBL', 'n:Oblique-erg', 'suffix'], ['NoLine#', 'юкъуз', 'n', '-з', '-з', 'DAT', 'n:SemCase', 'suffix']], [['NoLine#', 'зун', 'pers', 'зун', '@@@', '@@@', '@@@', None]], [['NoLine#', 'хуьряй', 'n', 'хуьр', 'хуьр', 'village', '<NotSure>', 'stem'], ['NoLine#', 'хуьряй', 'n', '-я', '-да', 'IN', 'n:InPreCase', 'suffix'], ['NoLine#', 'хуьряй', 'n', '-й', '-ай', 'EL', 'n:DirCASE', 'suffix']], [['NoLine#', 'кцӏариз', 'nprop', 'кцӏар', '@@@', '@@@', '@@@', None], ['NoLine#', 'кцӏариз', 'nprop', '-и', '-ди', 'OBL', 'n:Oblique-erg', 'suffix'], ['NoLine#', 'кцӏариз', 'nprop', '-з', '-з', 'DAT', 'n:SemCase', 'suffix']], [['NoLine#', 'хквезвай', 'v', 'хкв', 'хт', 'return', 'v(UEA)', 'stem'], ['NoLine#', 'хквезвай', 'v', '-езва', '-зава', 'IMPF', 'v:TMimpf', 'suffix'], ['NoLine#', 'хквезвай', 'v', '-й', '-й', 'PTCP', 'v:PTCP', 'suffix']], [['NoLine#', 'тир', 'cop', 'тир', 'тир', 'was', 'cop', 'stem']], [['NoLine#', 'рекъе', 'n', 'рекъ', 'рекъ', 'way,.road', 'n', 'stem'], ['NoLine#', 'рекъе', 'n', 'е', '@@@', '@@@', '@@@', None]], [['NoLine#', 'са', 'cardnum', 'са', 'са', 'one', 'cardnum', 'stem']], [['NoLine#', 'стӏурви', 'n', 'стӏурви', '@@@', '@@@', '@@@', None]], [['NoLine#', 'стхадал', 'n', 'стха', 'стха', 'brother', 'n', 'stem'], ['NoLine#', 'стхадал', 'n', '-да', '-да', 'OBL', 'n:inOBL', 'suffix'], ['NoLine#', 'стхадал', 'n', '-л', '-л', 'SP', 'n:SPpreCase', 'suffix'], ['NoLine#', 'стхадал', 'n', '-NULL', '-NULL', 'ESS', 'n:DirCASE', 'suffix']], [['NoLine#', 'гьалтна', '@@@', 'гьалт', '@@@', '@@@', '@@@', None], ['NoLine#', 'гьалтна', '@@@', 'на', '@@@', '@@@', '@@@', None]], [['NoLine#', 'таниш', 'adj', 'таниш', 'таниш', 'acquainted', 'adj', 'stem']], [['NoLine#', 'хьана', '@@@', 'хьа', '@@@', '@@@', '@@@', None], ['NoLine#', 'хьана', '@@@', '-ана', '-на', 'AOR', '@@@', 'suffix']], [['NoLine#', 'жув', 'proform', 'жув', 'жув', 'us', 'proform', 'stem']], [['NoLine#', 'чирна', 'v', 'чир', 'чир', 'knowledge', 'n', 'stem'], ['NoLine#', 'чирна', 'v', '-на', '-на', 'AOC', 'v:Mood&Cnvb', 'suffix']], [['NoLine#', 'гьадаз', 'pro', 'гьада', 'гьада', 'she', 'pro', 'stem'], ['NoLine#', 'гьадаз', 'pro', '-з', '-з', 'DAT', 'n:SemCase', 'suffix']], [['NoLine#', 'адаз', 'pro', 'ада', 'ада', 'he', 'pro', 'stem'], ['NoLine#', 'адаз', 'pro', '-з', '-з', 'DAT', 'n:SemCase', 'suffix']]]\n",
      "\n",
      "Total segmented and glossed lexemes: 11134\n",
      "\n",
      "[[['NoLine#', 'са', 'cardnum', 'са', 'са', 'one', 'cardnum', 'stem']], [['NoLine#', 'хуьряй', 'n', 'хуьр', 'хуьр', 'village', '<NotSure>', 'stem'], ['NoLine#', 'хуьряй', 'n', '-я', '-да', 'IN', 'n:InPreCase', 'suffix'], ['NoLine#', 'хуьряй', 'n', '-й', '-ай', 'EL', 'n:DirCASE', 'suffix']], [['NoLine#', 'хквезвай', 'v', 'хкв', 'хт', 'return', 'v(UEA)', 'stem'], ['NoLine#', 'хквезвай', 'v', '-езва', '-зава', 'IMPF', 'v:TMimpf', 'suffix'], ['NoLine#', 'хквезвай', 'v', '-й', '-й', 'PTCP', 'v:PTCP', 'suffix']], [['NoLine#', 'тир', 'cop', 'тир', 'тир', 'was', 'cop', 'stem']], [['NoLine#', 'са', 'cardnum', 'са', 'са', 'one', 'cardnum', 'stem']], [['NoLine#', 'стхадал', 'n', 'стха', 'стха', 'brother', 'n', 'stem'], ['NoLine#', 'стхадал', 'n', '-да', '-да', 'OBL', 'n:inOBL', 'suffix'], ['NoLine#', 'стхадал', 'n', '-л', '-л', 'SP', 'n:SPpreCase', 'suffix'], ['NoLine#', 'стхадал', 'n', '-NULL', '-NULL', 'ESS', 'n:DirCASE', 'suffix']], [['NoLine#', 'таниш', 'adj', 'таниш', 'таниш', 'acquainted', 'adj', 'stem']], [['NoLine#', 'жув', 'proform', 'жув', 'жув', 'us', 'proform', 'stem']], [['NoLine#', 'чирна', 'v', 'чир', 'чир', 'knowledge', 'n', 'stem'], ['NoLine#', 'чирна', 'v', '-на', '-на', 'AOC', 'v:Mood&Cnvb', 'suffix']], [['NoLine#', 'гьадаз', 'pro', 'гьада', 'гьада', 'she', 'pro', 'stem'], ['NoLine#', 'гьадаз', 'pro', '-з', '-з', 'DAT', 'n:SemCase', 'suffix']], [['NoLine#', 'адаз', 'pro', 'ада', 'ада', 'he', 'pro', 'stem'], ['NoLine#', 'адаз', 'pro', '-з', '-з', 'DAT', 'n:SemCase', 'suffix']], [['NoLine#', 'хьи', 'coordconn', 'хьи', 'хьи', 'that', 'coordconn', 'stem']], [['NoLine#', 'ки', 'subordconn', 'ки', 'ки', 'that', 'subordconn', 'stem']], [['NoLine#', 'журналист', 'n', 'журналист', 'журналист', 'journalist', 'n', 'stem']], [['NoLine#', 'ада', 'pro', 'ада', 'ада', 'he', 'pro', 'stem']], [['NoLine#', 'заз', 'pers', 'за', 'за', '1sg.ERG', 'pers', 'stem'], ['NoLine#', 'заз', 'pers', '-з', '-з', 'DAT', 'n:SemCase', 'suffix']], [['NoLine#', 'са', 'cardnum', 'са', 'са', 'one', 'cardnum', 'stem']], [['NoLine#', 'кьиса', 'n', 'кьиса', 'кьиса', 'story', 'n', 'stem']]]\n"
     ]
    }
   ],
   "source": [
    "datalocation = r\"../../../OneDrive - University of Florida/AL/data/\"\n",
    "flexdata = [r'./FLExtexts/lez-all_txts_2019.flextext', r'./FLExtexts/lez-all_txts_2022.flextext']\n",
    "\n",
    "for dbfile in flexdata:\n",
    "    name = os.path.basename(dbfile).split('.')[0]\n",
    "    print('\\n', name)\n",
    "    master_data = extract_flextext(dbfile)\n",
    "\n",
    "    # filter for my training purposes, split data lacking necessary annotations\n",
    "    # returns list of words\n",
    "    trainable_words, unannotated_words = quality_check(master_data)\n",
    "    print(trainable_words[:18])\n",
    "\n",
    "    # write all extracted words to _M(aster) file \n",
    "    # write unannotated data (for my purposes) to _U(nlabeled) file\n",
    "    # write annotated data to separate file\n",
    "\n",
    "    #extract2file(master_data, '_seg', datalocation+name+'_M')\n",
    "    extract2file(unannotated_words, '_surf_seg', datalocation+name+'_U')\n",
    "    extract2file(trainable_words, '_surf_seg', datalocation+name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
