{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET # parses XML files\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data from flextext XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "# placeholders/delimiters\n",
    "TEMP = '@@@'\n",
    "\n",
    "# text-level flextext XML attributes\n",
    "TITLE_TYPE = 'title'\n",
    "COMMENT_TYPE = 'comment'\n",
    "# flextext XML attributes for languages/scripts used in title or translations\n",
    "ENGLISH = 'en'\n",
    "INDONESIAN = 'id'\n",
    "\n",
    "# IGT tier-level flextext XML attributes\n",
    "TXT = 'txt' # surface morph/segment AND transcribed text\n",
    "CAN_MORPHEME = 'cf' # canonical (underlying) morpheme\n",
    "GLOSS = 'gls' # morpheme gloss AND sentence free translation\n",
    "M_POS = 'msa' # morpheme-level pos, what category affix attaches to\n",
    "WORD_POS = 'pos' # word-level pos\n",
    "PUNCT = 'punct' # punctuation\n",
    "\n",
    "# morpheme types (flextext XML attributes)\n",
    "MWE = 'phrase' # multiword expression\n",
    "PREFIX = 'prefix'\n",
    "SUFFIX = 'suffix'\n",
    "CIRCUMFIX = 'circumfix'\n",
    "PROCLITIC = 'proclitic'\n",
    "ENCLITICS = ['enclitic', 'clitic'] # NOTE: clitic functions as enclitic in some FLEx databases (e.g. lmk)\n",
    "INFIXES = ['infix', 'infixing interfix']\n",
    "STEMS = ['stem', 'bound stem', 'bound root', 'bound root A', 'root', 'particle']    \n",
    "\n",
    "# Segment boundaries are  uniquely marked in FLEx, add yours here\n",
    "# NOTE: FLEx databases handle circumfixes differently\n",
    "# NOTE: these symbols will need to be removed before re-importing to FLEx\n",
    "CIRCUM_PRE = '>'\n",
    "CIRCUM_POST = '<'\n",
    "CIRCUM_HOLE = '<>'\n",
    "CLITIC = '='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitleComment(xmlsection):\n",
    "    '''find title and comment if in this section\n",
    "    some documents have both and english and native language titles\n",
    "    these checks assure that the both will always be used if found separated by //\n",
    "    if only one of them is found then it is used\n",
    "    if none are found return NO TITLE FOUND'''\n",
    "    \n",
    "    title = \"NO TITLE FOUND\" \n",
    "    eng_title = TEMP\n",
    "    non_eng_title = TEMP\n",
    "    comment = \"No comment\"\n",
    "    \n",
    "    for item_lin in xmlsection.iter('item'):\n",
    "        if item_lin.get('type') == TITLE_TYPE and item_lin.get('lang') == ENGLISH:\n",
    "            eng_title = item_lin.text\n",
    "        if item_lin.get('type') == TITLE_TYPE and item_lin.get('lang') != ENGLISH:\n",
    "            non_eng_title = item_lin.text\n",
    "        if item_lin.get('type') == COMMENT_TYPE and item_lin.get('lang') == ENGLISH:\n",
    "            comment = item_lin.text\n",
    "    # check languages of title and add either or both\n",
    "    if eng_title != TEMP and non_eng_title == TEMP:\n",
    "        title = eng_title \n",
    "    elif eng_title == TEMP and non_eng_title != TEMP:\n",
    "        title = non_eng_title\n",
    "    elif eng_title != TEMP and non_eng_title != TEMP:\n",
    "        title = eng_title + ' // ' + non_eng_title \n",
    "        \n",
    "    return title, comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These cleaning functions handle pecularities or non-conventional annotation of IGT\n",
    "\n",
    "def cleanWord(IGTstring):\n",
    "    '''formats word to reduce confusion'''\n",
    "    \n",
    "    #TODO: reverse before reimporting to FLEx\n",
    "    \n",
    "    IGTstring = str(IGTstring)\n",
    "    \n",
    "    # TODO?: phrasal lexemes separated by double tilde\n",
    "    #IGTstring = IGTstring.strip().replace(' ', '~~')\n",
    "    # remove Cyrillic quotation mark hyphen\n",
    "    IGTstring = IGTstring.strip('-')\n",
    "    # use tilde for hyphenated words\n",
    "    IGTstring = IGTstring.replace('-', '~')\n",
    "    \n",
    "    return IGTstring.strip().lower()\n",
    "    \n",
    "    \n",
    "def cleanMorph(IGTstring):\n",
    "    '''remove unexpected symbols in surface morphs and canonical morphemes\n",
    "    (includes infixes and circumfix halves)'''\n",
    "    \n",
    "    #TODO: reverse before reimporting to FLEx\n",
    "    \n",
    "    # separate multiple words in morpheme string with period\n",
    "    IGTstring = IGTstring.replace(' ', '.')\n",
    "    \n",
    "    IGTstring = IGTstring.lower()\n",
    "    \n",
    "    # make null morpheme symbol consistent across databases, avoid encoding bugs\n",
    "    IGTstring = IGTstring.replace('Ø','NULL').replace('∅', 'NULL').replace('zero', 'NULL')\n",
    "    # add your null morpheme symbol here\n",
    "    IGTstring = IGTstring.replace('*0','NULL') # lez\n",
    "    \n",
    "    # NOTE: add here any pre-processing specific to a database\n",
    "    #IGTstring = IGTstring.replace('*', '') # NTU\n",
    "    \n",
    "    return IGTstring.strip()\n",
    "\n",
    "\n",
    "def cleanGloss(IGTstring, morpheme_type):\n",
    "    '''preprocess morpheme glosses\n",
    "    Follow Leipzig glossing rules where possible'''\n",
    "    \n",
    "    # separate multiple words in glosses with period, per linguistic convention\n",
    "    IGTstring = IGTstring.replace('-','.').replace(' ', '.')\n",
    "    \n",
    "    # make affix glosses all caps, per linguistic convention\n",
    "    if morpheme_type not in STEMS:\n",
    "        IGTstring = IGTstring.upper()\n",
    "    \n",
    "    return IGTstring.strip()\n",
    "\n",
    "\n",
    "def cleanPOS(IGTstring):\n",
    "    '''preprocess morpheme-level POS and word-level POS'''\n",
    "    \n",
    "    #TODO: reverse before returning to FLEx\n",
    "    \n",
    "    # separate multiple tags with period, per linguistic convention\n",
    "    IGTstring = IGTstring.replace(' ', '')\n",
    "    # remove FLEx-inserted hyphens, to reduce confusion w morpheme delimiter\n",
    "    #TODO: reverse before returning to FLEx\n",
    "    IGTstring = IGTstring.replace('pro-form', 'proform').replace('Nom-1','Nom1')\n",
    "    \n",
    "    # NOTE: add here any pre-processing specific to a database\n",
    "    IGTstring = IGTstring.replace('N (kx cl)', 'N(kx.cl)') ## Natugu [ntu] morpheme pos\n",
    "    \n",
    "    return IGTstring.strip()   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfixedStem(wordtxt, morphitem, infix):\n",
    "    '''infixed stems need special processing,\n",
    "    especially for non-neural models that require glosses for every segment'''\n",
    "    \n",
    "    pre_temp_morph = [TEMP, TEMP, TEMP, TEMP]\n",
    "    post_temp_morph = [TEMP, TEMP, TEMP, TEMP]\n",
    "    \n",
    "    infix = infix[0][1:-1] # remove dashes surrounding infixes\n",
    "    stemhalves = wordtxt.split(infix) # treat strings surrounding infixes as stems\n",
    "    \n",
    "    # get other tiers\n",
    "    for item in morphitem.iter('item'):\n",
    "        if item.get('type') != None or item.text != '' or item.text != '<NotSure>' or item.text != ' ':\n",
    "            # get surface morph, treat same as stem halves\n",
    "            if (item.get('type') == TXT):\n",
    "                pre_temp_morph[0] = cleanGloss(stemhalves[0])\n",
    "                post_temp_morph[0] = cleanGloss(stemhalves[1])\n",
    "            # canonical morpheme, will be nothing for first half if infixed\n",
    "            elif(item.get('type') == CAN_MORPHEME):\n",
    "                pre_temp_morph[1] = cleanMorph(item.text)\n",
    "                post_temp_morph[1] = cleanMorph(item.text)\n",
    "            # gloss, same for both\n",
    "            elif(item.get('type') == GLOSS):\n",
    "                # separate multi-word glosses with \".\"\n",
    "                pre_temp_morph[2] = cleanGloss(item.text)\n",
    "                post_temp_morph[2] = cleanGloss(item.text)\n",
    "            # morpheme pos\n",
    "            elif(item.get('type') == M_POS):\n",
    "                pre_temp_morph[3] = cleanPOS(item.text)\n",
    "                post_temp_morph[3] = cleanPOS(item.text)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return pre_temp_morph, post_temp_morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMorpheme(morphitem, morphemetype):\n",
    "    '''OUTPUT for each morpheme segment: [morph, morpheme, gloss, mpos]\n",
    "    To add more items to this array of info about morpheme segments:\n",
    "    1st. Add another holding place in the morph_info array; give index for that info piece.\n",
    "    2nd. Add elif statement for new tier using the attribute you want, e.g. 'morpheme type'.\n",
    "        If necessary, create special delimiter and write \"cleaning\" function.\n",
    "    3rd. Check that that morph_info array matches entries in temp_morph\n",
    "        and does not mess up punctuation processing.'''\n",
    "    \n",
    "    # temporary array for morpheme information\n",
    "    morph_info = [TEMP, TEMP, TEMP, TEMP, morphemetype]\n",
    "    # indexes for types of information to be in morph_info\n",
    "    MORPH_IDX = 0\n",
    "    MORPHEME_IDX = 1\n",
    "    GLOSS_IDX = 2\n",
    "    M_POS_IDX = 3\n",
    "    \n",
    "    # make uniform label for all stem-like morphemes\n",
    "    # assume missing morpheme type attribute is a stem\n",
    "    if morphemetype == None or morphemetype in STEMS:\n",
    "        morphemetype = 'stem'        \n",
    "    \n",
    "    # catch \"new\" morpheme types in current database\n",
    "    if (morphemetype not in STEMS and morphemetype not in INFIXES\n",
    "        and morphemetype != PROCLITIC and morphemetype not in ENCLITICS\n",
    "        and morphemetype != PREFIX and morphemetype != SUFFIX\n",
    "        and morphemetype != MWE and morphemetype != CIRCUMFIX):\n",
    "            print(\"\\nThis morpheme type XML attribute is not handled yet in getMorpheme(): \" + morphemetype)\n",
    "    \n",
    "    # extract information about morpheme from IGT tiers\n",
    "    for item in morphitem.iter('item'):\n",
    "        if item.text != None:\n",
    "            # surface morph (txt)\n",
    "            if (item.get('type') == TXT):\n",
    "                if morphemetype in ENCLITICS:\n",
    "                    morph_info[MORPH_IDX] = CLITIC + cleanMorph(item.text)\n",
    "                elif morphemetype == PROCLITIC:\n",
    "                    morph_info[MORPH_IDX] = cleanMorph(item.text) + CLITIC\n",
    "                else:\n",
    "                    morph_info[MORPH_IDX] = cleanMorph(item.text)\n",
    "            # TIER: canonical morpheme (cf)\n",
    "            elif(item.get('type') == CAN_MORPHEME):\n",
    "                # TODO: do not assume only 1 circumfix per word\n",
    "                if morphemetype == CIRCUMFIX: \n",
    "                    # if first half of circumfix is word-initial, treat as prefix\n",
    "                    if numaffix == 1:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text) + CIRCUM_PRE\n",
    "                    # if first half of circumfix is not word-initial, treat as infix\n",
    "                    else:\n",
    "                        morph_info[MORPHEME_IDX] = CIRCUM_POST + cleanMorph(item.text) + CIRCUM_PRE\n",
    "                # treat halves circumfix as pre/suffix, treat circumfixed stem as stem\n",
    "                elif '-...-' in item.text:\n",
    "                    if morphemetype in STEMS or morphemetype == MWE:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text).replace('-...-', '')\n",
    "                    elif morphemetype == PREFIX:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text).replace('-...-', CIRCUM_PRE)\n",
    "                    elif morphemetype == SUFFIX:\n",
    "                        morph_info[MORPHEME_IDX] = CIRCUM_POST + cleanMorph(item.text).replace('-...-', '')\n",
    "                # other canonical morpheme types\n",
    "                else:\n",
    "                    if morphemetype in ENCLITICS:\n",
    "                        morph_info[MORPHEME_IDX] = CLITIC + cleanMorph(item.text)\n",
    "                    elif morphemetype == PROCLITIC:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text) + CLITIC\n",
    "                    else:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text)\n",
    "            # TIER: gloss\n",
    "            elif (item.get('type') == GLOSS):\n",
    "                morph_info[GLOSS_IDX] = cleanGloss(item.text, morphemetype)\n",
    "            # TIER: morpheme pos\n",
    "            elif(item.get('type') == M_POS):\n",
    "                morph_info[M_POS_IDX] = cleanPOS(item.text)\n",
    "                \n",
    "    return morph_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_flextext(flextext_filename):\n",
    "    '''Takes FLExText XML any number of texts. OUTPUT: \n",
    "    [{\"text_title\":title, \"text_comment\":comment, \"words\":[\n",
    "        {\"segnum\":line#, \"orig_word\":word, \"POS\":postag, \"morphemes\": [\n",
    "                    [morph, morpheme, gloss, mpos, morphemetype]\n",
    "    ]}]}]'''\n",
    "\n",
    "    root = ET.parse(flextext_filename).getroot()\n",
    "    texts = []\n",
    "    total_lexemes = 0 # NOTE: MWE is 1 lexeme\n",
    "    pos_tags_in_corpus = set()\n",
    "    \n",
    "    for text in root.iter('interlinear-text'):\n",
    "        title,comment = getTitleComment(text)\n",
    "        temp_text = {\"text_title\":title, \"text_comment\":comment}\n",
    "        \n",
    "        # ignoring paragraph breaks\n",
    "        temp_words = []\n",
    "        for line_i,phrase in enumerate(text.iter('phrase')):\n",
    "            # FLEx \"segnum\" is ID for phrase/line/sentence\n",
    "            segnum = TEMP\n",
    "            if phrase.find('item').get('type') == 'segnum':\n",
    "                segnum = phrase.find('item').text\n",
    "            else:\n",
    "                segnum = str(line_i)\n",
    "\n",
    "            # \"words\" or MWE as tokenized by FLEx user\n",
    "            for word in phrase.iter('word'):\n",
    "                wordtype = word.find('item').get('type')\n",
    "                wordstring = cleanWord(word.find('item').text)\n",
    "                # ignore punctuation & digits\n",
    "                if wordtype != PUNCT and not wordstring.isdigit() and wordstring != '~' and wordstring != '':\n",
    "                    affix_order = [] # to align infixes \n",
    "                    total_lexemes+=1\n",
    "                    \n",
    "                    # get word POS\n",
    "                    temp_wpos = TEMP # word-level POS\n",
    "                    for word_item in word.iter('item'):\n",
    "                        if word_item.get('type') == WORD_POS:\n",
    "                            temp_wpos = cleanPOS(word_item.text)\n",
    "                    pos_tags_in_corpus.add(temp_wpos)\n",
    "                    \n",
    "                    # get interlinear for word segments, if any\n",
    "                    temp_morphemes = []\n",
    "                    if word.find('morphemes') == None:  #TODO?: eliminate this line, use filter function\n",
    "                        temp_morphemes.append([TEMP, TEMP, TEMP, TEMP, TEMP])\n",
    "                    else:\n",
    "                        for morph in word.iter('morph'):\n",
    "                            morphemetype = morph.get('type')\n",
    "                            \n",
    "                            # TODO: for non-neural models (need input/output alignment)\n",
    "                            # morpheme type will determine what part of string is infix\n",
    "                            #affix_order.append(morphemetype)\n",
    "                            # handle infixes\n",
    "                            #if len(affix_order) >= 2 and affix_order[-2] in infixes:\n",
    "                                # NOTE: FLEx seems to always put infix before its stem\n",
    "                                #preinfix, postinfix = getInfixedStem(str(wrd), morph, temp_word[-1])\n",
    "                                # insert first half of prefix for surface segmentation\n",
    "                                #infix_index = len(affix_order)-2\n",
    "                                #temp_word.insert(infix_index-1, preinfix)\n",
    "                                # add second half of infixed stem\n",
    "                                #temp_morph = postinfix\n",
    "                            #else:\n",
    "                            \n",
    "                            temp_morph = getMorpheme(morph, morphemetype)\n",
    "                            \n",
    "                            # Add generic gloss to unglossed proper nouns\n",
    "                            # check gloss index\n",
    "                            if temp_wpos == 'nprop' and morphemetype in STEMS:\n",
    "                                if temp_morph[2] == TEMP: \n",
    "                                    temp_morph[2] = 'proper_name'\n",
    "                            \n",
    "                            # add morpheme to dict of word's segments\n",
    "                            temp_morphemes.append(temp_morph)\n",
    "                    \n",
    "                    # create word dict\n",
    "                    temp_words.append({\"line#\":segnum,\"orig_word\":wordstring,\"POS\":temp_wpos,\"morphemes\":temp_morphemes})\n",
    "            \n",
    "            # TODO: get free translations\n",
    "            # TODO: handle as many languages if needed\n",
    "            #en_translation = TEMP\n",
    "            #id_translation = TEMP\n",
    "            #temp_phrase_gloss = [p_item for p_item in phrase.iter('item')]\n",
    "            # make sure the last item is indeed our phrase translation\n",
    "            #for tpg in temp_phrase_gloss:\n",
    "            #    if tpg.get('type') == gloss and tpg.get('lang') == ENGLISH:\n",
    "            #        en_translation = tpg.text\n",
    "            #    if tpg.get('type') == gloss and tpg.get('lang') == INDONESIAN:\n",
    "            #        id_translation = tpg.text\n",
    "            #append metadata the translation of the phrase to the end of the temp line\n",
    "            #temp_line.append(en_translation)\n",
    "            #temp_line.append(id_translation)\n",
    "            \n",
    "        \n",
    "        # add lines to text\n",
    "        temp_text[\"words\"] = temp_words\n",
    "        texts.append(temp_text)\n",
    "    \n",
    "    # corpus statistics\n",
    "    print(\"Part of speech found:\", pos_tags_in_corpus, end='\\n\\n')\n",
    "    print(\"Tokenized lexemes, ignoring punctuation and digits:\", total_lexemes, end='\\n\\n')\n",
    "    # sanity check first line\n",
    "    print(texts[0][\"words\"][:10])\n",
    "    print()\n",
    "                                \n",
    "    return texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering \n",
    "\n",
    "word_by_morpheme -> `texts[text_idx][\"words\"][word_idx][\"morphemes\"]`, i.e. [[morph, morpheme, gloss, mpos, morphemetype],...]\n",
    "\n",
    "lexical_item -> `texts[text_idx][words][word_idx][\"orig_word\"]`, i.e. wordstring\n",
    "\n",
    "#### Morpheme level Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glossed(word_by_morphemes):\n",
    "    '''no words with missing glosses;\n",
    "        assumes segmentation is complete'''\n",
    "    glossed = True\n",
    "    for segment in word_by_morphemes:\n",
    "        # check gloss of morphemes\n",
    "        if segment[2] == TEMP:\n",
    "            glossed = False\n",
    "            break # this line saves time \n",
    "    return glossed\n",
    "    \n",
    "def segmented(word_by_morphemes):\n",
    "    '''no words that have not been segmented \n",
    "    (i.e. no <morphemes> tag in XML)'''\n",
    "    annotated = True\n",
    "    if len(word_by_morphemes) == 1:\n",
    "        # check surface morpheme\n",
    "        if word_by_morphemes[0][0] == TEMP:\n",
    "            annotated = False\n",
    "        #TODO: check canonical morpheme\n",
    "    return annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word level filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiword(lexical_item):\n",
    "    '''no lexical items with spaces'''\n",
    "    mwe = False\n",
    "    # check original text of word\n",
    "    if ' ' in lexical_item or '~' in lexical_item or '-' in lexical_item:\n",
    "        mwe = True\n",
    "    return mwe\n",
    "\n",
    "def selected_pos(word_postag):\n",
    "    '''filter for a list of specified word level POS'''\n",
    "    undesired_pos = False\n",
    "    # check word level POS tag\n",
    "    if word_postag not in SELECT_POS_TAGS:\n",
    "            undesired_pos = True\n",
    "    return undesired_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMBINE FILTER FUNCTIONS HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_check(extractedtexts):\n",
    "    '''Write custom filter functions above, add calls here\n",
    "        add/remove function calls as needed. \n",
    "        Returns list of words as list of morphemes'''\n",
    "    \n",
    "    good = []\n",
    "    bad = []\n",
    "    good_cnt = 0\n",
    "    for text in extractedtexts:\n",
    "        for word in text[\"words\"]:\n",
    "            # add word level function to line below to completely eliminate\n",
    "            if not multiword(word['orig_word']) and selected_pos(word['POS']):\n",
    "                # add morpheme level filtering functions on line below\n",
    "                # use line below if purpose == gls or seg_gls\n",
    "                #if glossed(word[\"morphemes\"]) and annotated(word[\"morphemes\"]):\n",
    "                # use line below if seg only (not gls)\n",
    "                if segmented(word[\"morphemes\"]):\n",
    "                    good.append(word)\n",
    "                    good_cnt+=1\n",
    "                # add filtered out words to unlabeled dataset    \n",
    "                else:\n",
    "                    bad.append(word)\n",
    "\n",
    "    print(\"Total after filtering:\", good_cnt, end='\\n\\n')\n",
    "    return good,bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to files\n",
    "\n",
    "Get this list of words: \n",
    "\n",
    "`[{\"segnum\":line#, \"orig_word\":word, \"POS\":postag, \"morphemes\": [\n",
    "                            [morph, morpheme, gloss, mpos, morphemetype]\n",
    "                         ]}]` \n",
    "\n",
    "to files with one word per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_alignment(a, b):\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(\"morph(emes) and gloss must be same amount in a word\")\n",
    "        \n",
    "\n",
    "def morpho_words(extracted_words, purpose, outfilepath):\n",
    "    '''Writes two files: x and Y (data and annotations; input and output)\n",
    "    No text or line divisions.\n",
    "    Possible purposes: gls = glossing only, seg_gls = segmentation+glossing, pos = (word) POS tagging'''\n",
    "    \n",
    "    input_data = []\n",
    "    output_data = []\n",
    "    \n",
    "    for word in extracted_words: \n",
    "        # input string (x)\n",
    "        input_data.append(' '.join(word[\"orig_word\"])) # space between letters\n",
    "            \n",
    "        # output types (Y)\n",
    "        wPOS_tag = word[\"POS\"]\n",
    "        canonical_morphemes = []\n",
    "        surface_morphemes = []\n",
    "        morpheme_glosses = []\n",
    "        for morpheme in word[\"morphemes\"]:\n",
    "            surface_morphemes.append(morpheme[0])\n",
    "            canonical_morphemes.append(morpheme[1])\n",
    "            morpheme_glosses.append(morpheme[2])\n",
    "\n",
    "        # determines what will be written to output file\n",
    "        if purpose == 'pos':\n",
    "            output_data.append(wPOS_tag)\n",
    "        elif purpose == 'can_seg':\n",
    "            output_data.append(' '.join(canonical_morphemes))\n",
    "        elif purpose == 'surf_seg':\n",
    "            output_data.append(' '.join(surface_morphemes))\n",
    "        elif purpose == 'surf_seg_gls':\n",
    "            check_alignment(surface_morphemes, morpheme_glosses)\n",
    "            combined_seg_gls = []\n",
    "            for i, morpheme in enumerate(surface_morphemes):\n",
    "                combined_seg_gls.append(morpheme+'#'+morpheme_glosses[i])\n",
    "            output_data.append(' '.join(combined_seg_gls))\n",
    "        #TODO: purpose == 'can_seg_gls' must handle null morphemes\n",
    "        else:\n",
    "            print(\"Output format not found.\")\n",
    "\n",
    "    with open(outfilepath+\"_\"+purpose+'.input', 'w', encoding='utf8') as I, open(outfilepath+\"_\"+purpose+'.output', 'w', encoding='utf8') as O:\n",
    "        I.write('\\n'.join(input_data))\n",
    "        O.write('\\n'.join(output_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Run Code: Extract Surface Segmentation Data to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " lez-all_txts_2019\n",
      "Part of speech found: {'msd', 'pers', 'ptcp', 'verbprt', 'num', 'adv', 'conn', 'n', 'prt', 'nprop', 'dem', 'adj', 'det', 'subordconn', 'ordnum', 'post', 'Vnf', 'indfpro', 'prep', 'cardnum', 'coordconn', 'poss', 'multipnum', 'interj', 'v', 'Vocpart', 'Vf', 'pro', 'proform', 'interrog', 'recp', '@@@', 'emph'}\n",
      "\n",
      "Tokenized lexemes, ignoring punctuation and digits: 13950\n",
      "\n",
      "[{'line#': '0', 'orig_word': 'са', 'POS': 'cardnum', 'morphemes': [['са', 'са', 'one', 'cardnum', 'stem']]}, {'line#': '0', 'orig_word': 'юкъуз', 'POS': 'n', 'morphemes': [['юкъ', 'юкъ', '@@@', 'n', 'stem'], ['-у', '-ди', 'OBL', 'n:(OBLIQUEstem)', 'suffix'], ['-з', '-з', 'DAT', 'n:(CASE)', 'suffix']]}, {'line#': '0', 'orig_word': 'зун', 'POS': 'pers', 'morphemes': [['зун', 'зун', '1sg.abs', 'pers', 'stem']]}, {'line#': '0', 'orig_word': 'хуьряй', 'POS': 'n', 'morphemes': [['хуьр', 'хуьр', 'village', 'n', 'stem'], ['-ай', '-ай', 'INELAT', 'n:(OBLIQUEstem)', 'suffix']]}, {'line#': '0', 'orig_word': 'кцӏариз', 'POS': 'n', 'morphemes': [['кцӏар', 'кцӏар', 'gusar', 'n', 'stem'], ['-и', '-ди', 'OBL', 'n:(OBLIQUEstem)', 'suffix'], ['-з', '-з', 'DAT', 'n:(CASE)', 'suffix']]}, {'line#': '0', 'orig_word': 'хквезвай', 'POS': 'Vnf', 'morphemes': [['хкве', 'хквен', 'return', 'v', 'stem'], ['-зва', '-зва', 'IMPF', 'v:TAM1', 'suffix'], ['-й', '-й', 'PST', 'cop:(TAM)', 'suffix']]}, {'line#': '0', 'orig_word': 'тир', 'POS': 'v', 'morphemes': [['тир', 'тир', 'was', 'v', 'stem']]}, {'line#': '0', 'orig_word': 'рекъе', 'POS': 'n', 'morphemes': [['рекъ', 'рекъ', 'way,.road', 'n', 'stem'], ['-е', '-е', 'INESS', 'n:(OBLIQUEstem)', 'suffix']]}, {'line#': '0', 'orig_word': 'са', 'POS': 'cardnum', 'morphemes': [['са', 'са', 'one', 'cardnum', 'stem']]}, {'line#': '0', 'orig_word': 'стӏурви', 'POS': 'n', 'morphemes': [['стӏурви', 'стӏурви', 'from.Sudur', 'n', 'stem']]}]\n",
      "\n",
      "Total after filtering: 5994\n",
      "\n",
      "[{'line#': '0', 'orig_word': 'са', 'POS': 'cardnum', 'morphemes': [['са', 'са', 'one', 'cardnum', 'stem']]}, {'line#': '0', 'orig_word': 'зун', 'POS': 'pers', 'morphemes': [['зун', 'зун', '1sg.abs', 'pers', 'stem']]}, {'line#': '0', 'orig_word': 'са', 'POS': 'cardnum', 'morphemes': [['са', 'са', 'one', 'cardnum', 'stem']]}, {'line#': '1', 'orig_word': 'жув', 'POS': 'proform', 'morphemes': [['жув', 'жув', 'us', 'proform', 'stem']]}, {'line#': '1', 'orig_word': 'гьадаз', 'POS': 'pro', 'morphemes': [['гьада', 'гьада', 'she', 'pro', 'stem'], ['-з', '-з', 'DAT', 'n:(CASE)', 'suffix']]}, {'line#': '2', 'orig_word': 'адаз', 'POS': 'pro', 'morphemes': [['ада', 'ада', 'he', 'pro', 'stem'], ['-з', '-з', 'DAT', 'n:(CASE)', 'suffix']]}, {'line#': '2', 'orig_word': 'хьи', 'POS': 'coordconn', 'morphemes': [['хьи', 'хьи', 'that', 'coordconn', 'stem']]}, {'line#': '2', 'orig_word': 'зун', 'POS': 'pers', 'morphemes': [['зун', 'зун', '1sg.abs', 'pers', 'stem']]}, {'line#': '2', 'orig_word': 'ки', 'POS': 'subordconn', 'morphemes': [['ки', 'ки', 'that', 'subordconn', 'stem']]}, {'line#': '2', 'orig_word': 'зун', 'POS': 'pers', 'morphemes': [['зун', 'зун', '1sg.abs', 'pers', 'stem']]}]\n",
      "[{'line#': '1', 'orig_word': 'я', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'line#': '25', 'orig_word': 'ама', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'line#': '51', 'orig_word': 'я', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'line#': '0', 'orig_word': 'сороковой', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'line#': '3', 'orig_word': 'ван', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'line#': '9', 'orig_word': 'гьа', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'line#': '28', 'orig_word': 'ваъ', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'line#': '31', 'orig_word': 'марфадикай', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'line#': '41', 'orig_word': 'ваъ', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'line#': '42', 'orig_word': 'ун', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}]\n",
      "\n",
      " lez-all_txts_2022\n",
      "Part of speech found: {'pers', 'verbprt', 'num', 'adv', 'conn', 'n', 'prt', 'nprop', 'dem', 'cop', 'adj', 'det', 'subordconn', 'ordnum', 'post', 'indfpro', 'prep', 'cardnum', 'coordconn', 'poss', 'multipnum', 'interj', 'v', 'IMPV', 'Vocpart', 'pro', 'proform', 'interrog', 'recp', '@@@', 'emph'}\n",
      "\n",
      "Tokenized lexemes, ignoring punctuation and digits: 13953\n",
      "\n",
      "[{'line#': '0', 'orig_word': 'са', 'POS': 'cardnum', 'morphemes': [['са', 'са', 'one', 'cardnum', 'stem']]}, {'line#': '0', 'orig_word': 'юкъуз', 'POS': 'n', 'morphemes': [['юкъ', '@@@', '@@@', '@@@', None], ['-у', '-ди', 'OBL', 'n:Oblique-erg', 'suffix'], ['-з', '-з', 'DAT', 'n:SemCase', 'suffix']]}, {'line#': '0', 'orig_word': 'зун', 'POS': 'pers', 'morphemes': [['зун', '@@@', '@@@', '@@@', None]]}, {'line#': '0', 'orig_word': 'хуьряй', 'POS': 'n', 'morphemes': [['хуьр', 'хуьр', 'village', '<NotSure>', 'stem'], ['-я', '-да', 'IN', 'n:InPreCase', 'suffix'], ['-й', '-ай', 'EL', 'n:DirCASE', 'suffix']]}, {'line#': '0', 'orig_word': 'кцӏариз', 'POS': 'nprop', 'morphemes': [['кцӏар', '@@@', '@@@', '@@@', None], ['-и', '-ди', 'OBL', 'n:Oblique-erg', 'suffix'], ['-з', '-з', 'DAT', 'n:SemCase', 'suffix']]}, {'line#': '0', 'orig_word': 'хквезвай', 'POS': 'v', 'morphemes': [['хкв', 'хт', 'return', 'v(UEA)', 'stem'], ['-езва', '-зава', 'IMPF', 'v:TMimpf', 'suffix'], ['-й', '-й', 'PTCP', 'v:PTCP', 'suffix']]}, {'line#': '0', 'orig_word': 'тир', 'POS': 'cop', 'morphemes': [['тир', 'тир', 'was', 'cop', 'stem']]}, {'line#': '0', 'orig_word': 'рекъе', 'POS': 'n', 'morphemes': [['рекъ', 'рекъ', 'way,.road', 'n', 'stem'], ['е', '@@@', '@@@', '@@@', None]]}, {'line#': '0', 'orig_word': 'са', 'POS': 'cardnum', 'morphemes': [['са', 'са', 'one', 'cardnum', 'stem']]}, {'line#': '0', 'orig_word': 'стӏурви', 'POS': 'n', 'morphemes': [['стӏурви', '@@@', '@@@', '@@@', None]]}]\n",
      "\n",
      "Total after filtering: 6795\n",
      "\n",
      "[{'line#': '0', 'orig_word': 'са', 'POS': 'cardnum', 'morphemes': [['са', 'са', 'one', 'cardnum', 'stem']]}, {'line#': '0', 'orig_word': 'зун', 'POS': 'pers', 'morphemes': [['зун', '@@@', '@@@', '@@@', None]]}, {'line#': '0', 'orig_word': 'кцӏариз', 'POS': 'nprop', 'morphemes': [['кцӏар', '@@@', '@@@', '@@@', None], ['-и', '-ди', 'OBL', 'n:Oblique-erg', 'suffix'], ['-з', '-з', 'DAT', 'n:SemCase', 'suffix']]}, {'line#': '0', 'orig_word': 'са', 'POS': 'cardnum', 'morphemes': [['са', 'са', 'one', 'cardnum', 'stem']]}, {'line#': '0', 'orig_word': 'гьалтна', 'POS': '@@@', 'morphemes': [['гьалт', '@@@', '@@@', '@@@', None], ['на', '@@@', '@@@', '@@@', None]]}, {'line#': '1', 'orig_word': 'таниш', 'POS': 'adj', 'morphemes': [['таниш', 'таниш', 'acquainted', 'adj', 'stem']]}, {'line#': '1', 'orig_word': 'хьана', 'POS': '@@@', 'morphemes': [['хьа', '@@@', '@@@', '@@@', None], ['-ана', '-на', 'AOR', '@@@', 'suffix']]}, {'line#': '1', 'orig_word': 'жув', 'POS': 'proform', 'morphemes': [['жув', 'жув', 'us', 'proform', 'stem']]}, {'line#': '1', 'orig_word': 'гьадаз', 'POS': 'pro', 'morphemes': [['гьада', 'гьада', 'she', 'pro', 'stem'], ['-з', '-з', 'DAT', 'n:SemCase', 'suffix']]}, {'line#': '2', 'orig_word': 'адаз', 'POS': 'pro', 'morphemes': [['ада', 'ада', 'he', 'pro', 'stem'], ['-з', '-з', 'DAT', 'n:SemCase', 'suffix']]}]\n",
      "[{'line#': '1', 'orig_word': 'я', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'line#': '25', 'orig_word': 'ама', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'line#': '51', 'orig_word': 'я', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'line#': '24', 'orig_word': 'таран', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'line#': '32', 'orig_word': 'гафариз', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'line#': '32', 'orig_word': 'аламукьнава', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'line#': '14', 'orig_word': 'амукъназ', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'line#': '0', 'orig_word': 'сороковой', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'line#': '3', 'orig_word': 'mугьарибе', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'line#': '12', 'orig_word': 'ун', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}]\n"
     ]
    }
   ],
   "source": [
    "####### FOR FILTERING ####### \n",
    "# lezgi pos tags: {'ordnum', 'Vnf', 'num', 'indfpro', 'nprop', 'emph', 'Vocpart', 'proform', 'multipnum', 'prep', 'adv', 'post', 'ptcp', 'pers', 'verbprt', 'coordconn', 'adj', 'v', 'conn', 'poss', 'pro', 'prt', 'det', 'dem', 'interj', 'msd', 'subordconn', 'Vf', 'cardnum', 'n', 'interrog', 'recp'}\n",
    "SELECT_POS_TAGS = ['Vnf', 'v', 'msd', 'Vf','n','IMPV','cop']\n",
    "file_ext = '_NV'\n",
    "####### FILE LOCATIONS ####### \n",
    "\n",
    "datalocation = r\"../../../OneDrive - University of Florida/AL/data/\"\n",
    "flexdata = [r'./flextexts/lez-all_txts_2019.flextext', r'./flextexts/lez-all_txts_2022.flextext']\n",
    "# Possible purposes: gls = glossing only, seg_gls = segmentation+glossing, pos = (word) POS tagging\n",
    "\n",
    "####### EXTRACT #######\n",
    "for dbfile in flexdata:\n",
    "    name = os.path.basename(dbfile).split('.')[0]\n",
    "    print('\\n', name)\n",
    "    master_data = extract_flextext(dbfile)\n",
    "\n",
    "    # filter for my training purposes, split data lacking necessary annotations\n",
    "    # returns list of words\n",
    "    trainable_words, unannotated_words = quality_check(master_data)\n",
    "    print(trainable_words[:10])\n",
    "    print(unannotated_words[:10])\n",
    "\n",
    "    # write all extracted words to _M(aster) file \n",
    "    #extract2file(master_data, '_seg', datalocation+name+'_M')\n",
    "    # write unannotated data (for my purposes) to _U(nlabeled) file\n",
    "    morpho_words(unannotated_words, 'surf_seg', datalocation+name+'_U'+file_ext)\n",
    "    # write annotated data to separate file\n",
    "    morpho_words(trainable_words, 'surf_seg', datalocation+name+file_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
