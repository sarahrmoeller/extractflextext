{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET # parses XML files\n",
    "import string\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data from flextext XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders/delimiters\n",
    "TEMP = '@@@'\n",
    "\n",
    "# text-level flextext XML attributes\n",
    "TITLE_TYPE = 'title'\n",
    "COMMENT_TYPE = 'comment'\n",
    "# flextext XML attributes for languages/scripts used in title or translations\n",
    "ENGLISH = 'en'\n",
    "INDONESIAN = 'id'\n",
    "\n",
    "# IGT tier-level flextext XML attributes\n",
    "TXT = 'txt' # surface morph/segment AND transcribed text\n",
    "CAN_MORPHEME = 'cf' # canonical (underlying) morpheme\n",
    "GLOSS = 'gls' # morpheme gloss AND sentence free translation\n",
    "M_POS = 'msa' # morpheme-level pos, what category affix attaches to\n",
    "WORD_POS = 'pos' # word-level pos\n",
    "PUNCT = 'punct' # punctuation\n",
    "\n",
    "# morpheme types (flextext XML attributes)\n",
    "MWE = 'phrase' # multiword expression\n",
    "PREFIX = 'prefix'\n",
    "SUFFIX = 'suffix'\n",
    "CIRCUMFIX = 'circumfix'\n",
    "PROCLITIC = 'proclitic'\n",
    "ENCLITICS = ['enclitic', 'clitic'] # NOTE: clitic functions as enclitic in some FLEx databases (e.g. lmk)\n",
    "INFIXES = ['infix', 'infixing interfix']\n",
    "STEMS = ['stem', 'bound stem', 'bound root', 'bound root A', 'root', 'particle']    \n",
    "\n",
    "# Segment boundaries are  uniquely marked in FLEx, add yours here\n",
    "# NOTE: FLEx databases handle circumfixes differently\n",
    "# NOTE: these symbols will need to be removed before re-importing to FLEx\n",
    "CIRCUM_PRE = '>'\n",
    "CIRCUM_POST = '<'\n",
    "CIRCUM_HOLE = '<>'\n",
    "CLITIC = '='"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitleComment(xmlsection):\n",
    "    '''find title and comment if in this section\n",
    "    some documents have both and english and native language titles\n",
    "    these checks assure that the both will always be used if found separated by //\n",
    "    if only one of them is found then it is used\n",
    "    if none are found return NO TITLE FOUND'''\n",
    "    \n",
    "    title = \"NO TITLE FOUND\" \n",
    "    eng_title = TEMP\n",
    "    non_eng_title = TEMP\n",
    "    comment = \"No comment\"\n",
    "    \n",
    "    for item_lin in xmlsection.iter('item'):\n",
    "        if item_lin.get('type') == TITLE_TYPE and item_lin.get('lang') == ENGLISH:\n",
    "            eng_title = item_lin.text\n",
    "        if item_lin.get('type') == TITLE_TYPE and item_lin.get('lang') != ENGLISH:\n",
    "            non_eng_title = item_lin.text\n",
    "        if item_lin.get('type') == COMMENT_TYPE and item_lin.get('lang') == ENGLISH:\n",
    "            comment = item_lin.text\n",
    "    # check languages of title and add either or both\n",
    "    if eng_title != TEMP and non_eng_title == TEMP:\n",
    "        title = eng_title \n",
    "    elif eng_title == TEMP and non_eng_title != TEMP:\n",
    "        title = non_eng_title\n",
    "    elif eng_title != TEMP and non_eng_title != TEMP:\n",
    "        title = eng_title + ' // ' + non_eng_title \n",
    "        \n",
    "    return title, comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "'''These cleaning functions handle pecularities of a corpus or non-conventional IGT annotations'''\n",
    "#TODO: reverse before reimporting to FLEx\n",
    "\n",
    "def cleanWord(IGTstring):  \n",
    "    \n",
    "    IGTstring = str(IGTstring)\n",
    "    \n",
    "    # TODO?: phrasal lexemes separated by double tilde\n",
    "    #IGTstring = IGTstring.strip().replace(' ', '~~')\n",
    "    # remove hyphen when it is a Cyrillic quotation mark \n",
    "    IGTstring = IGTstring.strip('-')\n",
    "    # tilde in hyphenated words to reduce confusion with morpheme breaks\n",
    "    IGTstring = IGTstring.replace('-', '~')\n",
    "    \n",
    "    return IGTstring.strip().lower()\n",
    "    \n",
    "    \n",
    "def cleanMorph(IGTstring):\n",
    "    '''remove unexpected symbols in surface morphs and canonical morphemes\n",
    "    (includes infixes and circumfix halves)'''\n",
    "    \n",
    "    # separate multiple words in morpheme string with period\n",
    "    IGTstring = IGTstring.replace(' ', '.')\n",
    "    \n",
    "    IGTstring = IGTstring.lower()\n",
    "    \n",
    "    # make null morpheme symbol consistent across databases, avoid encoding bugs\n",
    "    IGTstring = IGTstring.replace('Ø','NULL').replace('∅', 'NULL').replace('zero', 'NULL')\n",
    "    # add your null morpheme symbol here\n",
    "    IGTstring = IGTstring.replace('*0','NULL') # lez\n",
    "    \n",
    "    # NOTE: add here any pre-processing specific to a database\n",
    "    #IGTstring = IGTstring.replace('*', '') # NTU\n",
    "    \n",
    "    return IGTstring.strip()\n",
    "\n",
    "\n",
    "def cleanGloss(IGTstring, morpheme_type):\n",
    "    '''preprocess morpheme glosses\n",
    "    Follow Leipzig glossing rules where possible'''\n",
    "    \n",
    "    # separate multiple words in glosses with period, per linguistic convention\n",
    "    IGTstring = IGTstring.replace('-','.').replace(' ', '.')\n",
    "    \n",
    "    # make affix glosses all caps, per linguistic convention\n",
    "    if morpheme_type not in STEMS:\n",
    "        IGTstring = IGTstring.upper()\n",
    "    \n",
    "    return IGTstring.strip()\n",
    "\n",
    "\n",
    "def cleanPOS(IGTstring):\n",
    "    '''preprocess morpheme-level POS and word-level POS'''\n",
    "    \n",
    "    #TODO: reverse before returning to FLEx\n",
    "    \n",
    "    # separate multiple tags with period, per linguistic convention\n",
    "    IGTstring = IGTstring.replace(' ', '')\n",
    "    # remove FLEx-inserted hyphens, to reduce confusion w morpheme delimiter\n",
    "    #TODO: reverse before returning to FLEx\n",
    "    IGTstring = IGTstring.replace('pro-form', 'proform').replace('Nom-1','Nom1')\n",
    "    \n",
    "    ### NOTE: add here any pre-processing specific to a database\n",
    "    IGTstring = IGTstring.replace('N (kx cl)', 'N(kx.cl)') ## Natugu [ntu] morpheme pos\n",
    "    \n",
    "    return IGTstring.strip()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfixedStem(wordtxt, morphitem, infix):\n",
    "    '''infixed stems need special processing,\n",
    "    especially for non-neural models that require glosses for every segment'''\n",
    "    \n",
    "    pre_temp_morph = [TEMP, TEMP, TEMP, TEMP]\n",
    "    post_temp_morph = [TEMP, TEMP, TEMP, TEMP]\n",
    "    \n",
    "    infix = infix[0][1:-1] # remove dashes surrounding infixes\n",
    "    stemhalves = wordtxt.split(infix) # treat strings surrounding infixes as stems\n",
    "    \n",
    "    # get other tiers\n",
    "    for item in morphitem.iter('item'):\n",
    "        if item.get('type') != None or item.text != '' or item.text != '<NotSure>' or item.text != ' ':\n",
    "            # get surface morph, treat same as stem halves\n",
    "            if (item.get('type') == TXT):\n",
    "                pre_temp_morph[0] = cleanGloss(stemhalves[0])\n",
    "                post_temp_morph[0] = cleanGloss(stemhalves[1])\n",
    "            # canonical morpheme, will be nothing for first half if infixed\n",
    "            elif(item.get('type') == CAN_MORPHEME):\n",
    "                pre_temp_morph[1] = cleanMorph(item.text)\n",
    "                post_temp_morph[1] = cleanMorph(item.text)\n",
    "            # gloss, same for both\n",
    "            elif(item.get('type') == GLOSS):\n",
    "                # separate multi-word glosses with \".\"\n",
    "                pre_temp_morph[2] = cleanGloss(item.text)\n",
    "                post_temp_morph[2] = cleanGloss(item.text)\n",
    "            # morpheme pos\n",
    "            elif(item.get('type') == M_POS):\n",
    "                pre_temp_morph[3] = cleanPOS(item.text)\n",
    "                post_temp_morph[3] = cleanPOS(item.text)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return pre_temp_morph, post_temp_morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMorpheme(morphitem, morphemetype, numaffix):\n",
    "    '''OUTPUT for each morpheme segment: [morph, morpheme, gloss, mpos]\n",
    "    To add more items to this array of info about morpheme segments:\n",
    "    1st. Add another holding place in the morph_info array; give index for that info piece.\n",
    "    2nd. Add elif statement for new tier using the attribute you want, e.g. 'morpheme type'.\n",
    "        If necessary, create special delimiter and write \"cleaning\" function.\n",
    "    3rd. Check that that morph_info array matches entries in temp_morph\n",
    "        and does not mess up punctuation processing.'''\n",
    "    \n",
    "    # temporary array for morpheme information\n",
    "    morph_info = [TEMP, TEMP, TEMP, TEMP, morphemetype]\n",
    "    # indexes for types of information to be in morph_info\n",
    "    MORPH_IDX = 0\n",
    "    MORPHEME_IDX = 1\n",
    "    GLOSS_IDX = 2\n",
    "    M_POS_IDX = 3\n",
    "    \n",
    "    # make uniform label for all stem-like morphemes\n",
    "    # assume missing morpheme type attribute is a stem\n",
    "    if morphemetype == None or morphemetype in STEMS:\n",
    "        morphemetype = 'stem'        \n",
    "    \n",
    "    # catch \"new\" morpheme types in current database\n",
    "    if (morphemetype not in STEMS and morphemetype not in INFIXES\n",
    "        and morphemetype != PROCLITIC and morphemetype not in ENCLITICS\n",
    "        and morphemetype != PREFIX and morphemetype != SUFFIX\n",
    "        and morphemetype != MWE and morphemetype != CIRCUMFIX):\n",
    "            print(\"\\nThis morpheme type XML attribute is not handled yet in getMorpheme(): \" + morphemetype)\n",
    "    \n",
    "    # extract information about morpheme from IGT tiers\n",
    "    for item in morphitem.iter('item'):\n",
    "        if item.text != None:\n",
    "            # surface morph (txt)\n",
    "            if (item.get('type') == TXT):\n",
    "                if morphemetype in ENCLITICS:\n",
    "                    morph_info[MORPH_IDX] = CLITIC + cleanMorph(item.text)\n",
    "                elif morphemetype == PROCLITIC:\n",
    "                    morph_info[MORPH_IDX] = cleanMorph(item.text) + CLITIC\n",
    "                else:\n",
    "                    morph_info[MORPH_IDX] = cleanMorph(item.text)\n",
    "            # TIER: canonical morpheme (cf)\n",
    "            elif(item.get('type') == CAN_MORPHEME):\n",
    "                # TODO: do not assume only 1 circumfix per word\n",
    "                if morphemetype == CIRCUMFIX: \n",
    "                    # if first half of circumfix is word-initial, treat as prefix\n",
    "                    if len(numaffix) == 1:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text) + CIRCUM_PRE\n",
    "                    # if first half of circumfix is not word-initial, treat as infix\n",
    "                    else:\n",
    "                        morph_info[MORPHEME_IDX] = CIRCUM_POST + cleanMorph(item.text) + CIRCUM_PRE\n",
    "                # treat halves circumfix as pre/suffix, treat circumfixed stem as stem\n",
    "                elif '-...-' in item.text:\n",
    "                    if morphemetype in STEMS or morphemetype == MWE:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text).replace('-...-', '')\n",
    "                    elif morphemetype == PREFIX:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text).replace('-...-', CIRCUM_PRE)\n",
    "                    elif morphemetype == SUFFIX:\n",
    "                        morph_info[MORPHEME_IDX] = CIRCUM_POST + cleanMorph(item.text).replace('-...-', '')\n",
    "                # other canonical morpheme types\n",
    "                else:\n",
    "                    if morphemetype in ENCLITICS:\n",
    "                        morph_info[MORPHEME_IDX] = CLITIC + cleanMorph(item.text)\n",
    "                    elif morphemetype == PROCLITIC:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text) + CLITIC\n",
    "                    else:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text)\n",
    "            # TIER: gloss\n",
    "            elif (item.get('type') == GLOSS):\n",
    "                morph_info[GLOSS_IDX] = cleanGloss(item.text, morphemetype)\n",
    "            # TIER: morpheme pos\n",
    "            elif(item.get('type') == M_POS):\n",
    "                morph_info[M_POS_IDX] = cleanPOS(item.text)\n",
    "                \n",
    "    return morph_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_flextext(flextext_filename):\n",
    "    '''Takes FLExText XML any number of texts. OUTPUT list of line dicts: \n",
    "    [{\"text_title\":title, \"text_comment\":comment, \"lineid\":line#, \"line_txt\":words_digits_string, words\":[\n",
    "            {\"orig_word\":word, \"POS\":postag, \"morphemes\":[\n",
    "                [morph, morpheme, gloss, mpos, morphemetype]\n",
    "    ]}]}]'''\n",
    "    \n",
    "    #TODO: add \"orig_line\":line\n",
    "\n",
    "    root = ET.parse(flextext_filename).getroot()\n",
    "    lines = []\n",
    "    total_lexemes = 0 # NOTE: MWE is 1 lexeme\n",
    "    pos_tags_in_corpus = set()\n",
    "    \n",
    "    for text in root.iter('interlinear-text'):\n",
    "        title,comment = getTitleComment(text)\n",
    "        \n",
    "        # ignoring paragraph breaks\n",
    "        for line_i,phrase in enumerate(text.iter('phrase')):\n",
    "            temp_line = {}\n",
    "            temp_words = []\n",
    "            no_punct_line = ''\n",
    "            # FLEx \"segnum\" is ID for phrase/line/sentence\n",
    "            segnum = TEMP\n",
    "            if phrase.find('item').get('type') == 'segnum':\n",
    "                segnum = phrase.find('item').text\n",
    "            else:\n",
    "                segnum = str(line_i)\n",
    "\n",
    "            # \"words\" or MWE as tokenized by FLEx user\n",
    "            for word in phrase.iter('word'):\n",
    "                wordtype = word.find('item').get('type')\n",
    "                wordstring = cleanWord(word.find('item').text)\n",
    "                # ignore punctuation & digits\n",
    "                #if wordtype != PUNCT and not wordstring.isdigit() and wordstring != '~' and wordstring != '':\n",
    "                # ignore punctuation only\n",
    "                if wordtype != PUNCT and wordstring != '~' and wordstring != '':\n",
    "                    temp_morphemes = []\n",
    "                    affix_order = [] # to align infixes \n",
    "                    no_punct_line += wordstring\n",
    "                    total_lexemes+=1\n",
    "                    \n",
    "                    # get word POS\n",
    "                    temp_wpos = TEMP # word-level POS\n",
    "                    for word_item in word.iter('item'):\n",
    "                        if word_item.get('type') == WORD_POS:\n",
    "                            temp_wpos = cleanPOS(word_item.text)\n",
    "                    # generic POS for digits\n",
    "                    if wordstring.isdigit():\n",
    "                        temp_wpos = 'num'\n",
    "                    pos_tags_in_corpus.add(temp_wpos)\n",
    "                    \n",
    "                    # get interlinear for word segments, if any\n",
    "                    if word.find('morphemes') == None:  #TODO?: eliminate this line, use filter function\n",
    "                        temp_morphemes.append([TEMP, TEMP, TEMP, TEMP, TEMP])\n",
    "                    else:\n",
    "                        for morph in word.iter('morph'):\n",
    "                            morphemetype = morph.get('type')\n",
    "                            \n",
    "                            # TODO: for non-neural models (need input/output alignment)\n",
    "                            # morpheme type will determine what part of string is infix\n",
    "                            affix_order.append(morphemetype)\n",
    "                            \n",
    "                            # handle infixes\n",
    "                            #if len(affix_order) >= 2 and affix_order[-2] in infixes:\n",
    "                                # NOTE: FLEx seems to always put infix before its stem\n",
    "                                #preinfix, postinfix = getInfixedStem(str(wrd), morph, temp_word[-1])\n",
    "                                # insert first half of prefix for surface segmentation\n",
    "                                #infix_index = len(affix_order)-2\n",
    "                                #temp_word.insert(infix_index-1, preinfix)\n",
    "                                # add second half of infixed stem\n",
    "                                #temp_morph = postinfix\n",
    "                            #else:\n",
    "                            \n",
    "                            temp_morph = getMorpheme(morph, morphemetype, affix_order)\n",
    "                            \n",
    "                            # Add generic gloss to unglossed proper nouns\n",
    "                            # check gloss index\n",
    "                            if temp_wpos == 'nprop' and morphemetype in STEMS:\n",
    "                                if temp_morph[2] == TEMP: \n",
    "                                    temp_morph[2] = 'proper_name'\n",
    "                            \n",
    "                            # add morpheme to dict of word's segments\n",
    "                            temp_morphemes.append(temp_morph)\n",
    "                    \n",
    "                    # create word dict\n",
    "                    temp_words.append({\"orig_word\":wordstring,\"POS\":temp_wpos,\"morphemes\":temp_morphemes})\n",
    "            \n",
    "            # TODO: get free translations\n",
    "            # TODO: handle as many languages if needed\n",
    "            #en_translation = TEMP\n",
    "            #id_translation = TEMP\n",
    "            #temp_phrase_gloss = [p_item for p_item in phrase.iter('item')]\n",
    "            # make sure the last item is indeed our phrase translation\n",
    "            #for tpg in temp_phrase_gloss:\n",
    "            #    if tpg.get('type') == gloss and tpg.get('lang') == ENGLISH:\n",
    "            #        en_translation = tpg.text\n",
    "            #    if tpg.get('type') == gloss and tpg.get('lang') == INDONESIAN:\n",
    "            #        id_translation = tpg.text\n",
    "            #append metadata the translation of the phrase to the end of the temp line\n",
    "            #temp_line.append(en_translation)\n",
    "            #temp_line.append(id_translation)\n",
    "            \n",
    "        \n",
    "            # add line\n",
    "            temp_line = {\"text_title\":title, \"text_comment\":comment, \"lineid\":segnum, \"line_txt\":no_punct_line, \"words\":temp_words}\n",
    "            lines.append(temp_line)\n",
    "    \n",
    "    # corpus statistics\n",
    "    print(\"Part of speech found:\", pos_tags_in_corpus, end='\\n\\n')\n",
    "    print(\"Tokenized lexemes, ignoring punctuation and digits:\", total_lexemes, end='\\n\\n')\n",
    "    # sanity check first line\n",
    "    print(lines[0][\"words\"][:10])\n",
    "    print()\n",
    "                                \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering \n",
    "\n",
    "word_by_morpheme -> `lines[\"words\"][word_idx][\"morphemes\"]`, i.e. [[morph, morpheme, gloss, mpos, morphemetype],...]\n",
    "\n",
    "lexical_item -> `lines[words][word_idx][\"orig_word\"]`, i.e. wordstring\n",
    "\n",
    "#### Morpheme level Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glossed(word_by_morphemes):\n",
    "    '''no words with missing glosses;\n",
    "        assumes segmentation is complete'''\n",
    "    \n",
    "    glossed = True\n",
    "    for segment in word_by_morphemes:\n",
    "        # check gloss of morphemes\n",
    "        if segment[2] == TEMP:\n",
    "            glossed = False\n",
    "            break # this line saves time \n",
    "    return glossed\n",
    "    \n",
    "    \n",
    "def surf_segmented(word_by_morphemes):\n",
    "    '''no words that have not been segmented \n",
    "    (i.e. no <morphemes> tag in XML)'''\n",
    "    \n",
    "    annotated = True\n",
    "    if len(word_by_morphemes) == 1:\n",
    "        # check surface morpheme\n",
    "        if word_by_morphemes[0][0] == TEMP:\n",
    "            annotated = False\n",
    "        #TODO: check canonical morpheme\n",
    "    return annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word level filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiword(lexical_item):\n",
    "    '''no lexical items with spaces'''\n",
    "    \n",
    "    mwe = False\n",
    "    # check original text of word\n",
    "    if ' ' in lexical_item or '~' in lexical_item or '-' in lexical_item:\n",
    "        mwe = True\n",
    "    return mwe\n",
    "\n",
    "\n",
    "def selected_pos(word_postag):\n",
    "    '''filter for a list of specified word level POS'''\n",
    "    \n",
    "    undesired_pos = False\n",
    "    # check word level POS tag\n",
    "    if word_postag not in SELECT_POS_TAGS:\n",
    "            undesired_pos = True\n",
    "    return undesired_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMBINE FILTER FUNCTIONS HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_check(extractedtexts):\n",
    "    '''Write custom filter functions above, add calls here\n",
    "        add/remove function calls as needed. \n",
    "        Returns list of words as list of morphemes'''\n",
    "    \n",
    "    good = []\n",
    "    bad = []\n",
    "    good_cnt = 0\n",
    "    for line in extractedtexts:\n",
    "        for word in line[\"words\"]:\n",
    "            ### add word level function  to completely eliminate a filtered word/POS\n",
    "            # uncomment line below if filtering for specific POS tags\n",
    "            #if not multiword(word['orig_word']) and selected_pos(word['POS']) and word['POS'] == 'num':\n",
    "            if not multiword(word['orig_word']) and not word['POS'] == 'num':\n",
    "                ### add morpheme level filtering functions below, use only one line ###\n",
    "                # uncomment line below if purpose is gls or seggls (not seg only)\n",
    "                #if glossed(word[\"morphemes\"]) and surf_segmented(word[\"morphemes\"]):\n",
    "                # uncomment line below if surface seg only (not gls)\n",
    "                #if surf_segmented(word[\"morphemes\"]):\n",
    "                # uncomment line below if using all three\n",
    "                if glossed(word[\"morphemes\"]) and surf_segmented(word[\"morphemes\"]) and surf_segmented(word[\"morphemes\"]):\n",
    "                    good.append(word)\n",
    "                    good_cnt+=1\n",
    "                # filtered words go to unlabeled dataset    \n",
    "                else:\n",
    "                    bad.append(word)\n",
    "\n",
    "    print(\"Total after filtering:\", good_cnt, end='\\n\\n')\n",
    "    return good,bad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to files\n",
    "\n",
    "Get this list of words:  \n",
    "\n",
    "Current: `[{\"text_title\":title, \"text_comment\":comment, \"lineid\":line#, \"words\":[\n",
    "            {\"orig_word\":word, \"POS\":postag, \"morphemes\":[\n",
    "                [morph, morpheme, gloss, mpos, morphemetype]\n",
    "    ]}]}]` \n",
    "\n",
    "Old: `[{\"text_title\":title, \"text_comment\":comment, \"words\":[\n",
    "            {\"segnum\":line#, \"orig_word\":word, \"POS\":postag, \"morphemes\": [\n",
    "                            [morph, morpheme, gloss, mpos, morphemetype]\n",
    "                         ]}]` \n",
    "\n",
    "to files with one word per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_alignment(a, b):\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(\"morph(emes) and gloss must be same amount in a word\")\n",
    "        \n",
    "\n",
    "def dataFiles(extracted_words, purpose, outfilepath):\n",
    "    '''Writes two files: x and Y (data and annotations; input and output)\n",
    "    No text or line divisions.\n",
    "    Possible purposes: gls = glossing only, seg_gls = segmentation+glossing, pos = (word) POS tagging'''\n",
    "    \n",
    "    input_data = []\n",
    "    output_data = []\n",
    "    \n",
    "    for word in extracted_words: \n",
    "        # input string (x)\n",
    "        input_data.append(' '.join(word[\"orig_word\"])) # space between letters\n",
    "            \n",
    "        # output types (Y)\n",
    "        wPOS_tag = word[\"POS\"]\n",
    "        canonical_morphemes = []\n",
    "        surface_morphemes = []\n",
    "        morpheme_glosses = []\n",
    "        for morpheme in word[\"morphemes\"]:\n",
    "            surface_morphemes.append(morpheme[0])\n",
    "            canonical_morphemes.append(morpheme[1])\n",
    "            morpheme_glosses.append(morpheme[2])\n",
    "\n",
    "        # determines what will be written to output file\n",
    "        #TODO: purpose == _canSegGls & _canSeg must handle null morphemes\n",
    "        if purpose == '_pos':\n",
    "            output_data.append(wPOS_tag)\n",
    "        elif purpose == '_gls':\n",
    "            output_data.append(' '.join(morpheme_glosses))\n",
    "        elif purpose == '_canSeg':\n",
    "            output_data.append(' '.join(canonical_morphemes))\n",
    "        elif purpose == '_surSeg':\n",
    "            output_data.append(' '.join(surface_morphemes))\n",
    "        elif purpose == '_surSegGls':\n",
    "            check_alignment(surface_morphemes, morpheme_glosses)\n",
    "            combined_seg_gls = []\n",
    "            for i, morpheme in enumerate(surface_morphemes):\n",
    "                combined_seg_gls.append(morpheme+'#'+morpheme_glosses[i])\n",
    "            output_data.append(' '.join(combined_seg_gls))\n",
    "        else:\n",
    "            print(\"Output format not found.\")\n",
    "\n",
    "    with open(outfilepath+purpose+'.input', 'w', encoding='utf8') as I, open(outfilepath+purpose+'.output', 'w', encoding='utf8') as O:\n",
    "        I.write('\\n'.join(input_data))\n",
    "        O.write('\\n'.join(output_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Run Code: Extract Surface Segmentation Data to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####### EXTRACT #######\n",
    "def main(dbfile, tasks):\n",
    "    ####### FOR FILTERING ####### \n",
    "    # lezgi pos tags: {'ordnum', 'Vnf', 'num', 'indfpro', 'nprop', 'emph', 'Vocpart', 'proform', 'multipnum', 'prep', 'adv', 'post', 'ptcp', 'pers', 'verbprt', 'coordconn', 'adj', 'v', 'conn', 'poss', 'pro', 'prt', 'det', 'dem', 'interj', 'msd', 'subordconn', 'Vf', 'cardnum', 'n', 'interrog', 'recp'}\n",
    "    # Alas pos tags: {'num', 'n', 'refl', 'Aux', 'vt', 'cop', 'clf', 'adv', 'prt', 'Adj', 'cardnum', 'vi', 'stc', 'existmrkr', 'quant', 'relpro', 'ordnum', 'vd', 'distrnum', 'adj', 'Prep', 'nprop', 'interj', 'Conj', 'dem', 'v', 'pro'}\n",
    "    #SELECT_POS_TAGS = ['Vnf', 'v', 'msd', 'Vf','n','IMPV','cop']\n",
    "    file_ext = ''\n",
    "\n",
    "    ####### FILE LOCATIONS ####### \n",
    "    LANG = 'ntu'\n",
    "    datalocation = r\"../../../OneDrive - University of Florida/AL/data/\"+LANG+'/'\n",
    "    to_extract = [r'./flextexts/'+LANG+'-all_txts.flextext']\n",
    "    #TASKS = ['_canSeg', '_surSeg', '_gls', '_canSegGls', '_surSegGls', '_pos']\n",
    "\n",
    "    datalines = extract_flextext(dbfile)\n",
    "\n",
    "    # filter for my training purposes, split data lacking necessary annotations\n",
    "    # returns list of words\n",
    "    trainable_words, unannotated_words = quality_check(datalines)\n",
    "    print(trainable_words[:10])\n",
    "    print(unannotated_words[:10])\n",
    "    \n",
    "    for task in tasks:\n",
    "        # write all extracted words to _M(aster) file \n",
    "        #extract2file(master_data, '_seg', datalocation+name+'_M')\n",
    "        # write unannotated data (for my purposes) to _U(nlabeled) file\n",
    "        dataFiles(unannotated_words, task, datalocation+LANG+'_U'+file_ext)\n",
    "        # write annotated data to separate file\n",
    "        dataFiles(trainable_words, task, datalocation+LANG+'_L'+file_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part of speech found: {'num', 'nomprt', 'imp', 'coordconn', 'n', 'verbprt', 'adv', 'mod', 'cardnum', 'proform', 'advlizer', 'quant', 'dir', 'post', '@@@', 'DM', 'adj', 'onom', 'inter', 'nprop', 'interj', 'nvp', 'dem', 'v', 'pro'}\n",
      "\n",
      "Tokenized lexemes, ignoring punctuation and digits: 14104\n",
      "\n",
      "[{'orig_word': 'keey', 'POS': 'n', 'morphemes': [['keey', 'keey', 'village', 'n', 'stem']]}, {'orig_word': 'tah', 'POS': 'post', 'morphemes': [['tah', 'tah', 'at:AR', 'post', 'stem']]}, {'orig_word': 'hihneeshyąą', 'POS': 'v', 'morphemes': [['hih-', 'h-', '3PL.S.', 'v:Any', 'prefix'], ['nee-', 'nee-', 'QUAL:DH.PFV:Ø.', 'Verb', 'prefix'], ['shyąą', 'shyąą', 'grow:PFV', 'v', 'stem']]}, {'orig_word': 'jah', 'POS': 'adv', 'morphemes': [['jah', 'jah', 'here', 'adv', 'stem']]}, {'orig_word': 'dineh', 'POS': '@@@', 'morphemes': [['dineh', 'dineh', 'person', 'n', 'stem']]}, {'orig_word': 'huuniign', 'POS': 'v', 'morphemes': [['huu-', 'huu-', '3SG.S:QUAL:DH.PFV:Ø.', 'Verb', 'prefix'], ['niign', 'niign', 'grab:PFV:NOM', 'v', 'stem']]}, {'orig_word': \"u'aat\", 'POS': 'n', 'morphemes': [['u-', 'u-', '3SG.PSR.', 'Noun', 'prefix'], [\"'aat\", \"'aat\", 'wife', 'n', 'stem']]}, {'orig_word': \"ts'exeh\", 'POS': 'n', 'morphemes': [[\"ts'exeh\", \"ts'exeh\", 'woman', 'n', 'stem']]}, {'orig_word': 'nadįhtįį', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}]\n",
      "\n",
      "Total after filtering: 11476\n",
      "\n",
      "[{'orig_word': 'keey', 'POS': 'n', 'morphemes': [['keey', 'keey', 'village', 'n', 'stem']]}, {'orig_word': 'tah', 'POS': 'post', 'morphemes': [['tah', 'tah', 'at:AR', 'post', 'stem']]}, {'orig_word': 'hihneeshyąą', 'POS': 'v', 'morphemes': [['hih-', 'h-', '3PL.S.', 'v:Any', 'prefix'], ['nee-', 'nee-', 'QUAL:DH.PFV:Ø.', 'Verb', 'prefix'], ['shyąą', 'shyąą', 'grow:PFV', 'v', 'stem']]}, {'orig_word': 'jah', 'POS': 'adv', 'morphemes': [['jah', 'jah', 'here', 'adv', 'stem']]}, {'orig_word': 'dineh', 'POS': '@@@', 'morphemes': [['dineh', 'dineh', 'person', 'n', 'stem']]}, {'orig_word': 'huuniign', 'POS': 'v', 'morphemes': [['huu-', 'huu-', '3SG.S:QUAL:DH.PFV:Ø.', 'Verb', 'prefix'], ['niign', 'niign', 'grab:PFV:NOM', 'v', 'stem']]}, {'orig_word': \"u'aat\", 'POS': 'n', 'morphemes': [['u-', 'u-', '3SG.PSR.', 'Noun', 'prefix'], [\"'aat\", \"'aat\", 'wife', 'n', 'stem']]}, {'orig_word': \"ts'exeh\", 'POS': 'n', 'morphemes': [[\"ts'exeh\", \"ts'exeh\", 'woman', 'n', 'stem']]}, {'orig_word': 'dineh', 'POS': '@@@', 'morphemes': [['dineh', 'dineh', 'person', 'n', 'stem']]}, {'orig_word': 'gaay', 'POS': 'mod', 'morphemes': [['gaay', 'gaay', 'small', 'mod', 'stem']]}]\n",
      "[{'orig_word': 'nadįhtįį', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'orig_word': 'ninįhtįį', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'orig_word': 'nihthoo', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'orig_word': 'nuhxon', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'orig_word': 'nuhhogndag', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'orig_word': 'ntsuul', 'POS': '@@@', 'morphemes': [['n-', 'n-', '@@@', 'Verb', 'prefix'], ['tsuul', 'tsuul', '@@@', 'v', 'stem']]}, {'orig_word': \"uk'e'\", 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'orig_word': 'įhtsuul', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'orig_word': 'ah', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}, {'orig_word': 'uchil', 'POS': '@@@', 'morphemes': [['@@@', '@@@', '@@@', '@@@', '@@@']]}]\n"
     ]
    }
   ],
   "source": [
    "main(to_extract[0], ['_surSeg', '_gls', '_surSegGls', '_pos'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
