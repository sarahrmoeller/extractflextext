{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import xml.etree.ElementTree as ET # parses XML files\n",
    "    import string\n",
    "    import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Data from flextext XML files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placeholders/delimiters\n",
    "TEMP = '@@@'\n",
    "\n",
    "# text-level flextext XML attributes\n",
    "TITLE_TYPE = 'title'\n",
    "COMMENT_TYPE = 'comment'\n",
    "# flextext XML attributes for languages/scripts used in title or translations\n",
    "ENGLISH = 'en'\n",
    "INDONESIAN = 'id'\n",
    "\n",
    "# IGT tier-level flextext XML attributes\n",
    "TXT = 'txt' # surface morph/segment AND transcribed text\n",
    "CAN_MORPHEME = 'cf' # canonical (underlying) morpheme\n",
    "GLOSS = 'gls' # morpheme gloss AND sentence free translation\n",
    "M_POS = 'msa' # morpheme-level pos, what category affix attaches to\n",
    "WORD_POS = 'pos' # word-level pos\n",
    "PUNCT = 'punct' # punctuation\n",
    "\n",
    "# morpheme types (flextext XML attributes)\n",
    "MWE = 'phrase' # multiword expression\n",
    "PREFIX = 'prefix'\n",
    "SUFFIX = 'suffix'\n",
    "CIRCUMFIX = 'circumfix'\n",
    "PROCLITIC = 'proclitic'\n",
    "ENCLITICS = ['enclitic', 'clitic'] # NOTE: clitic functions as enclitic in some FLEx databases (e.g. lmk)\n",
    "INFIXES = ['infix', 'infixing interfix']\n",
    "STEMS = ['stem', 'bound stem', 'bound root', 'bound root A', 'root', 'particle']\n",
    "COMPOUND2 = 'bound root B'\n",
    "\n",
    "# Segment boundaries are  uniquely marked in FLEx, add yours here\n",
    "# NOTE: FLEx databases handle circumfixes differently\n",
    "# NOTE: these symbols will need to be removed before re-importing to FLEx\n",
    "CIRCUM_PRE = '>'\n",
    "CIRCUM_POST = '<'\n",
    "CIRCUM_HOLE = '<>'\n",
    "CLITIC = '='\n",
    "BOUNDROOT = '*'\n",
    "\n",
    "# Generic GLOSSes\n",
    "PROPER_NOUN_GLOSS = 'NPROP'\n",
    "DIGIT_GLOSS_POS = 'NUM'\n",
    "\n",
    "# Output dictionary keys\n",
    "TITLE = 'text_title'\n",
    "COMMENT = 'text_comment'\n",
    "SEGNUM = 'line#'\n",
    "FT = 'free_transl'\n",
    "ORIG_LINE = 'orig_line'\n",
    "WORDS = 'words'\n",
    "TOKEN = 'token'\n",
    "POS = 'wPOS'\n",
    "MORPHEMES = \"morphemes\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTitleComment(xmlsection):\n",
    "    '''find title and comment if in this section\n",
    "    some documents have both and english and native language titles\n",
    "    these checks assure that the both will always be used if found separated by //\n",
    "    if only one of them is found then it is used\n",
    "    if none are found return NO TITLE FOUND'''\n",
    "    \n",
    "    title = \"NO TITLE FOUND\" \n",
    "    eng_title = TEMP\n",
    "    non_eng_title = TEMP\n",
    "    comment = \"No comment\"\n",
    "    \n",
    "    for item_lin in xmlsection.iter('item'):\n",
    "        if item_lin.get('type') == TITLE_TYPE and item_lin.get('lang') == ENGLISH:\n",
    "            eng_title = item_lin.text\n",
    "        if item_lin.get('type') == TITLE_TYPE and item_lin.get('lang') != ENGLISH:\n",
    "            non_eng_title = item_lin.text\n",
    "        if item_lin.get('type') == COMMENT_TYPE and item_lin.get('lang') == ENGLISH:\n",
    "            comment = item_lin.text\n",
    "    # check languages of title and add either or both\n",
    "    if eng_title != TEMP and non_eng_title == TEMP:\n",
    "        title = eng_title \n",
    "    elif eng_title == TEMP and non_eng_title != TEMP:\n",
    "        title = non_eng_title\n",
    "    elif eng_title != TEMP and non_eng_title != TEMP:\n",
    "        title = eng_title + ' // ' + non_eng_title \n",
    "        \n",
    "    return title, comment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################\n",
    "'''These cleaning functions handle pecularities of a corpus or non-conventional IGT annotations'''\n",
    "\n",
    "def cleanWord(IGTstring):  \n",
    "    \n",
    "    IGTstring = str(IGTstring)\n",
    "    \n",
    "    # TODO?: phrasal lexemes to be separated by double tilde\n",
    "    #IGTstring = IGTstring.strip().replace(' ', '~~')\n",
    "    # Strip hyphens from words. This handles hyphen as Cyrillic quotation mark \n",
    "    IGTstring = IGTstring.strip('-')\n",
    "    # Use tilde in hyphenated words. Don't confuse w hyphen as morpheme breaks\n",
    "    IGTstring = IGTstring.replace('-', '~')\n",
    "    \n",
    "    return IGTstring.strip().lower()\n",
    "    \n",
    "    \n",
    "def cleanMorph(IGTstring):\n",
    "    '''remove unexpected symbols in surface morphs and canonical morphemes\n",
    "    (includes infixes and circumfix halves)'''\n",
    "    \n",
    "    # separate multiple words in morpheme string with period\n",
    "    IGTstring = IGTstring.replace(' ', '.')\n",
    "    \n",
    "    IGTstring = IGTstring.lower()\n",
    "    \n",
    "    # make null morpheme symbol consistent across databases, avoid encoding bugs\n",
    "    IGTstring = IGTstring.replace('Ø','NULL').replace('∅', 'NULL').replace('zero', 'NULL')\n",
    "    # add your null morpheme symbol here\n",
    "    IGTstring = IGTstring.replace('*0','NULL') # lez\n",
    "    # make * on bound roots into suffix hyphens\n",
    "    IGTstring = IGTstring.replace('*', '-') # ntu\n",
    "    \n",
    "    # NOTE: add here any pre-processing specific to a database\n",
    "    #IGTstring = IGTstring.replace('*', '') # NTU\n",
    "    \n",
    "    return IGTstring.strip()\n",
    "\n",
    "\n",
    "def cleanGloss(IGTstring, morpheme_type):\n",
    "    '''preprocess morpheme glosses\n",
    "    Follow Leipzig glossing rules where possible'''\n",
    "    \n",
    "    # separate multiple words in glosses with period, per linguistic convention\n",
    "    IGTstring = IGTstring.replace('-','.').replace(' ', '.')\n",
    "    \n",
    "    # make affix glosses all caps, per linguistic convention\n",
    "    if morpheme_type not in STEMS:\n",
    "        IGTstring = IGTstring.upper()\n",
    "    \n",
    "    return IGTstring.strip()\n",
    "\n",
    "\n",
    "def cleanPOS(IGTstring):\n",
    "    '''preprocess morpheme-level POS and word-level POS'''\n",
    "    \n",
    "    #TODO: reverse before returning to FLEx\n",
    "    \n",
    "    # separate multiple tags with period, per linguistic convention\n",
    "    IGTstring = IGTstring.replace(' ', '')\n",
    "    # remove FLEx-inserted hyphens, to reduce confusion w morpheme delimiter\n",
    "    #TODO: reverse before returning to FLEx\n",
    "    IGTstring = IGTstring.replace('pro-form', 'proform').replace('Nom-1','Nom1')\n",
    "    \n",
    "    ### NOTE: add here any pre-processing specific to a database\n",
    "    IGTstring = IGTstring.replace('N (kx cl)', 'N(kx.cl)') ## Natugu [ntu] morpheme pos\n",
    "    \n",
    "    return IGTstring.strip()   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getInfixedStem(wordtxt, morphitem, infix):\n",
    "    '''infixed stems need special processing,\n",
    "    especially for non-neural models that require glosses for every segment'''\n",
    "    \n",
    "    pre_temp_morph = [TEMP, TEMP, TEMP, TEMP]\n",
    "    post_temp_morph = [TEMP, TEMP, TEMP, TEMP]\n",
    "    \n",
    "    infix = infix[0][1:-1] # remove dashes surrounding infixes\n",
    "    stemhalves = wordtxt.split(infix) # treat strings surrounding infixes as stems\n",
    "    \n",
    "    # get other tiers\n",
    "    for item in morphitem.iter('item'):\n",
    "        if item.get('type') != None or item.text != '' or item.text != '<NotSure>' or item.text != ' ':\n",
    "            # get surface morph, treat same as stem halves\n",
    "            if (item.get('type') == TXT):\n",
    "                pre_temp_morph[0] = cleanGloss(stemhalves[0])\n",
    "                post_temp_morph[0] = cleanGloss(stemhalves[1])\n",
    "            # canonical morpheme, will be nothing for first half if infixed\n",
    "            elif(item.get('type') == CAN_MORPHEME):\n",
    "                pre_temp_morph[1] = cleanMorph(item.text)\n",
    "                post_temp_morph[1] = cleanMorph(item.text)\n",
    "            # gloss, same for both\n",
    "            elif(item.get('type') == GLOSS):\n",
    "                # separate multi-word glosses with \".\"\n",
    "                pre_temp_morph[2] = cleanGloss(item.text)\n",
    "                post_temp_morph[2] = cleanGloss(item.text)\n",
    "            # morpheme pos\n",
    "            elif(item.get('type') == M_POS):\n",
    "                pre_temp_morph[3] = cleanPOS(item.text)\n",
    "                post_temp_morph[3] = cleanPOS(item.text)\n",
    "        else:\n",
    "            continue\n",
    "    \n",
    "    return pre_temp_morph, post_temp_morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMorpheme(morphitem, morphemetype, numaffix):\n",
    "    '''OUTPUT for each morpheme segment: [morph, morpheme, gloss, mpos, morphemetype]\n",
    "    \n",
    "    To add more items to this array of info about morpheme segments:\n",
    "    1st. Add another holding place in the morph_info array; give index for that info piece.\n",
    "    2nd. Add elif statement for new tier using the attribute you want, e.g. 'morpheme type'.\n",
    "        If necessary, create special delimiter and write \"cleaning\" function.\n",
    "    3rd. Check that that morph_info array matches entries in temp_morph\n",
    "        and does not mess up punctuation processing.'''\n",
    "    \n",
    "    # temporary array for morpheme information\n",
    "    morph_info = [TEMP, TEMP, TEMP, TEMP, morphemetype]\n",
    "    # indexes for types of information to be in morph_info\n",
    "    MORPH_IDX = 0\n",
    "    MORPHEME_IDX = 1\n",
    "    GLOSS_IDX = 2\n",
    "    M_POS_IDX = 3\n",
    "    TYPE_IDX = 4\n",
    "    \n",
    "    # make uniform label for all stem-like morphemes\n",
    "    # assume missing morpheme type attribute is a stem\n",
    "    if morphemetype == None or morphemetype in STEMS:\n",
    "        morphemetype = 'stem'\n",
    "        \n",
    "    # make 2nd half of compound stems (bound root B) into suffixes (derivational)\n",
    "    if morphemetype == COMPOUND2:\n",
    "        morphemetype = 'suffix'\n",
    "    \n",
    "    # catch \"new\" morpheme types in current database\n",
    "    if (morphemetype not in STEMS and morphemetype not in INFIXES\n",
    "        and morphemetype != PROCLITIC and morphemetype not in ENCLITICS\n",
    "        and morphemetype != PREFIX and morphemetype != SUFFIX\n",
    "        and morphemetype != MWE and morphemetype != CIRCUMFIX\n",
    "        and morphemetype != COMPOUND2):\n",
    "            print(\"\\nThis morpheme type XML attribute is not handled yet in getMorpheme(): \" + morphemetype)\n",
    "    \n",
    "    # extract information about morpheme from IGT tiers\n",
    "    for item in morphitem.iter('item'):\n",
    "        if item.text != None:\n",
    "            # TIER => surface morph (txt)\n",
    "            if (item.get('type') == TXT):\n",
    "                if morphemetype in ENCLITICS:\n",
    "                    morph_info[MORPH_IDX] = CLITIC + cleanMorph(item.text)\n",
    "                elif morphemetype == PROCLITIC:\n",
    "                    morph_info[MORPH_IDX] = cleanMorph(item.text) + CLITIC\n",
    "                else:\n",
    "                    morph_info[MORPH_IDX] = cleanMorph(item.text)\n",
    "            # TIER => canonical morpheme (cf)\n",
    "            elif(item.get('type') == CAN_MORPHEME):\n",
    "                # TODO: do not assume only 1 circumfix per word\n",
    "                if morphemetype == CIRCUMFIX: \n",
    "                    # if first half of circumfix is word-initial, treat as prefix\n",
    "                    if len(numaffix) == 1:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text) + CIRCUM_PRE\n",
    "                    # if first half of circumfix is not word-initial, treat as infix\n",
    "                    else:\n",
    "                        morph_info[MORPHEME_IDX] = CIRCUM_POST + cleanMorph(item.text) + CIRCUM_PRE\n",
    "                # treat circumfix halves as pre/suffix, treat circumfixed stem as stem\n",
    "                elif '-...-' in item.text:\n",
    "                    if morphemetype in STEMS or morphemetype == MWE:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text).replace('-...-', '')\n",
    "                    elif morphemetype == PREFIX:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text).replace('-...-', CIRCUM_PRE)\n",
    "                    elif morphemetype == SUFFIX:\n",
    "                        morph_info[MORPHEME_IDX] = CIRCUM_POST + cleanMorph(item.text).replace('-...-', '')\n",
    "                # other canonical morpheme types\n",
    "                else:\n",
    "                    if morphemetype in ENCLITICS:\n",
    "                        morph_info[MORPHEME_IDX] = CLITIC + cleanMorph(item.text)\n",
    "                    elif morphemetype == PROCLITIC:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text) + CLITIC\n",
    "                    else:\n",
    "                        morph_info[MORPHEME_IDX] = cleanMorph(item.text)\n",
    "            # TIER => gloss\n",
    "            elif (item.get('type') == GLOSS):\n",
    "                morph_info[GLOSS_IDX] = cleanGloss(item.text, morphemetype)\n",
    "            # TIER: morpheme pos\n",
    "            elif(item.get('type') == M_POS):\n",
    "                morph_info[M_POS_IDX] = cleanPOS(item.text)\n",
    "                \n",
    "    return morph_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getWPOS(current_tokenXML, current_token, current_token_type):\n",
    "    temp_wpos = TEMP\n",
    "    \n",
    "    for word_item in current_tokenXML.iter('item'):\n",
    "        if word_item.get('type') == WORD_POS:\n",
    "            temp_wpos = cleanPOS(word_item.text)\n",
    "    # generic POS for digits\n",
    "    if current_token.isdigit():\n",
    "        temp_wpos = DIGIT_GLOSS_POS\n",
    "    # generic POS for punctuation\n",
    "    if current_token_type == PUNCT or current_token == '~':\n",
    "        temp_wpos = PUNCT.upper()\n",
    "    \n",
    "    return temp_wpos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Extraction Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_flextext(flextext_filename):\n",
    "    '''Takes FLExText XML any number of texts. OUTPUT list of line dicts: \n",
    "    [{title, comment, line#, free_transl, origline_w_digits_punct, words\":[\n",
    "                {word_txt, wpostag, \"morphemes\":[\n",
    "                    [morph, morpheme, gloss, mpos, morphemetype]\n",
    "    ]}]}]'''\n",
    "\n",
    "    root = ET.parse(flextext_filename).getroot()\n",
    "    lines = []\n",
    "    total_lexemes = 0 #No punct or digits. NOTE: MWE is 1 lexeme\n",
    "    total_tokens = 0\n",
    "    pos_tags_in_corpus = set()\n",
    "    \n",
    "    for text in root.iter('interlinear-text'):\n",
    "        title,comment = getTitleComment(text)\n",
    "        \n",
    "        # This gets info at the phrase (sentence/line), word, morpheme level, ignores paragraph breaks\n",
    "        for line_idx,phrase in enumerate(text.iter('phrase')):\n",
    "            temp_line = {}\n",
    "            temp_words = []\n",
    "            no_punct_line = ''\n",
    "            orig_line = []\n",
    "\n",
    "            # FLExtext \"segnum\" is ID for phrases\n",
    "            if phrase.find('item').get('type') == 'segnum':\n",
    "                lineid = phrase.find('item').text\n",
    "            else:\n",
    "                lineid = str(line_idx)                \n",
    "            \n",
    "            # Get free translations\n",
    "            # TODO: handle as many languages if needed\n",
    "            temp_transl = TEMP\n",
    "            if phrase.find('item').get('type') == GLOSS:\n",
    "                if phrase.find('item').get('lang') == ENGLISH:\n",
    "                    temp_transl = phrase.find('item').text\n",
    "                else:\n",
    "                    temp_transl = phrase.find('item').text\n",
    "            # This gets token (word) level info, as tokenized by FLEx user. \n",
    "            # Note: MWE (phrasal lexems) are one \"word\"\n",
    "            for token_idx,token in enumerate(phrase.iter('word')):\n",
    "                tokentype = token.find('item').get('type')\n",
    "                token_string = cleanWord(token.find('item').text)\n",
    "                \n",
    "                # Uncomment line below to ignore punctuation (but not digits)\n",
    "                #if tokentype != PUNCT and token_string != '~' and token_string != '':\n",
    "                # Uncomment line below to ignore punctuation & digits \n",
    "                #if tokentype != PUNCT and not token_string.isdigit() and token_string != '~' and token_string != '':\n",
    "                # Uncomment line below to keep punctuation and digits, only ignore empty items\n",
    "                if token_string != '':\n",
    "                    total_tokens+=1\n",
    "                    orig_line.append(token_string)\n",
    "                    temp_morphemes = []\n",
    "                    affix_order = [] # order of affixes to align infixes later         \n",
    "                    # get total lexemes \n",
    "                    if tokentype != PUNCT and token_string != '~' and not token_string.isdigit():\n",
    "                        no_punct_line += token_string\n",
    "                        total_lexemes+=1    \n",
    "                    \n",
    "                    # get word-level POS\n",
    "                    temp_wpos = getWPOS(token, token_string, tokentype)\n",
    "                    pos_tags_in_corpus.add(temp_wpos)\n",
    "                    \n",
    "                    # get interlinear tiers for word segments, if any\n",
    "                    if token.find('morphemes') == None:  #TODO?: eliminate this line, use filter function\n",
    "                        temp_morphemes.append([token_string, token_string, TEMP, temp_wpos, tokentype])\n",
    "                    else:\n",
    "                        for morph in token.iter('morph'):\n",
    "                            morphemetype = morph.get('type')\n",
    "                            \n",
    "                            # TODO: for non-neural models (need input/output alignment)\n",
    "                            # morpheme type will determine what part of string is infix\n",
    "                            affix_order.append(morphemetype)\n",
    "                            \n",
    "                            # Handle infixes\n",
    "                            #if len(affix_order) >= 2 and affix_order[-2] in infixes:\n",
    "                                # NOTE: FLEx seems to always put infix before its stem\n",
    "                                #preinfix, postinfix = getInfixedStem(str(wrd), morph, temp_word[-1])\n",
    "                                # insert first half of prefix for surface segmentation\n",
    "                                #infix_index = len(affix_order)-2\n",
    "                                #temp_word.insert(infix_index-1, preinfix)\n",
    "                                # add second half of infixed stem\n",
    "                                #temp_morph = postinfix\n",
    "                            #else: \n",
    "                            temp_morph = getMorpheme(morph, morphemetype, affix_order)\n",
    "                            \n",
    "                            # Add generic gloss to unglossed proper nouns\n",
    "                            if temp_wpos == 'nprop' and morphemetype in STEMS:\n",
    "                                if temp_morph[2] == TEMP: \n",
    "                                    temp_morph[2] = PROPER_NOUN_GLOSS\n",
    "                            \n",
    "                            # add morpheme to dict of word's segments\n",
    "                            temp_morphemes.append(temp_morph)\n",
    "                    \n",
    "                    # create word dict\n",
    "                    temp_words.append({TOKEN:token_string, POS:temp_wpos, MORPHEMES:temp_morphemes})            \n",
    "        \n",
    "            # add line\n",
    "            orig_line = ' '.join(orig_line)\n",
    "            temp_line = {TITLE:title, COMMENT:comment, SEGNUM:lineid, FT:temp_transl, ORIG_LINE:orig_line, WORDS:temp_words}\n",
    "            lines.append(temp_line)\n",
    "    \n",
    "    # corpus statistics\n",
    "    print(\"Parts of speech found in corpus:\", pos_tags_in_corpus, end='\\n\\n')\n",
    "    print(\"All Tokens:\", total_tokens)\n",
    "    print(\"Lexemes, ignoring punctuation and digits:\", total_lexemes, end='\\n\\n')\n",
    "    # sanity check first line\n",
    "    print(lines[0][WORDS][:10])\n",
    "    print()\n",
    "                                \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering \n",
    "\n",
    "word_by_morpheme -> `lines[\"words\"][word_idx][\"morphemes\"]`, i.e. [[morph, morpheme, gloss, mpos, morphemetype],...]\n",
    "\n",
    "lexical_item -> `lines[words][word_idx][\"orig_word\"]`, i.e. wordstring\n",
    "\n",
    "#### Morpheme level Filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def glossed(word_by_morphemes):\n",
    "    '''no words with missing glosses;\n",
    "        assumes segmentation is complete'''\n",
    "    glossed = True\n",
    "    for segment in word_by_morphemes:\n",
    "        # check gloss of morphemes\n",
    "        if segment[2] == TEMP:\n",
    "            glossed = False\n",
    "            break # this line saves time \n",
    "    return glossed\n",
    "    \n",
    "    \n",
    "def surf_segmented(word_by_morphemes):\n",
    "    '''no words that have not been segmented \n",
    "    (i.e. no <morphemes> tag in XML)'''\n",
    "    annotated = True\n",
    "    if len(word_by_morphemes) == 1:\n",
    "        if word_by_morphemes[0][0] == TEMP:\n",
    "            annotated = False\n",
    "    return annotated\n",
    "\n",
    "\n",
    "def can_segmented(word_by_morphemes):\n",
    "    '''no words that have not been canonically segmented'''\n",
    "    annotated = True\n",
    "    if len(word_by_morphemes) == 1:\n",
    "        if word_by_morphemes[0][1] == TEMP:\n",
    "            annotated = False\n",
    "    return annotated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word level filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiword(lexical_item):\n",
    "    '''no lexical items with spaces'''\n",
    "    mwe = False\n",
    "    # check original text of word\n",
    "    if ' ' in lexical_item or '~' in lexical_item or '-' in lexical_item:\n",
    "        mwe = True\n",
    "    return mwe\n",
    "\n",
    "\n",
    "def selected_pos(word_postag):\n",
    "    '''filter for a list of specified word level POS'''\n",
    "    #SELECT_POS_TAGS = ['Vnf', 'v', 'msd', 'Vf','n','IMPV','cop']\n",
    "    SELECT_POS_TAGS = []\n",
    "    undesired_pos = False\n",
    "    # check word level POS tag\n",
    "    if word_postag not in SELECT_POS_TAGS:\n",
    "            undesired_pos = True\n",
    "    return undesired_pos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### COMBINE FILTER FUNCTIONS HERE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filtering(extractedtexts, bysentence):\n",
    "    '''Write custom filter functions above, add calls here\n",
    "        add/remove function calls as needed.\n",
    "        Returns list of word dictionaries'''\n",
    "    \n",
    "    for_training = []\n",
    "    non_training = []\n",
    "    for line in extractedtexts:\n",
    "        linewords = []\n",
    "        linestatus = [] # to check for any filtered tokens\n",
    "        temp_for_training = []\n",
    "        temp_non_training = [] \n",
    "        \n",
    "        for word in line[WORDS]:\n",
    "            good4training = True\n",
    "            \n",
    "            '''Uncomment if statements to add word level filtering\n",
    "              that filters certain word types or POS tags'''\n",
    "            # To filter out MWE\n",
    "            #if multiword(word['orig_word']): good4training = False\n",
    "            # To filter out unselected, unspecified POS\n",
    "            #if not selected_pos(word['POS']): good4training = False\n",
    "            # To filter out digits\n",
    "            #if word['POS'] == DIGIT_GLOSS_POS: good4training = False\n",
    "            # To filter out punctuation\n",
    "            #if word['POS'] == PUNCT: good4training = False\n",
    "            \n",
    "            '''Uncomment if statements to filter for specific annotation'''\n",
    "            if good4training:\n",
    "                # For training to gloss\n",
    "                #if not glossed(word[MORPHEMES]): good4training = False\n",
    "                # For training to surface segment\n",
    "                #if not surf_segmented(word[MORPHEMES]): good4training = False\n",
    "                # For training surface segmentation\n",
    "                #if not surf_segmented(word[MORPHEMES]): good4training = False\n",
    "                # For training canonical (underlying) segmentation\n",
    "                #if not can_segmented(word[MORPHEMES]): good4training = False\n",
    "                # For training POS tagging\n",
    "                if word[POS] == TEMP: good4training = False\n",
    "                \n",
    "            linewords.append(word)\n",
    "            linestatus.append(good4training)\n",
    "            if good4training:\n",
    "                temp_for_training.append(word)\n",
    "            else:\n",
    "                temp_non_training.append(word)\n",
    "        \n",
    "        # For training by sentences w/o filtered tokens\n",
    "        if bysentence:\n",
    "            # End of sentence marker\n",
    "            EOS = {TOKEN:'EOS', POS:'@EOS@', MORPHEMES:['@EOS@', '@EOS@', '@EOS@', '@EOS@', '@EOS@']}\n",
    "            linewords.append(EOS)\n",
    "            if all(linestatus): \n",
    "                for_training.extend(linewords)\n",
    "            else:\n",
    "                non_training.extend(linewords)\n",
    "        # For training by word, not sentence \n",
    "        else:\n",
    "            if good4training:\n",
    "                for_training.extend(temp_for_training)\n",
    "            else:\n",
    "                non_training.extend(temp_non_training)\n",
    "                \n",
    "    print(\"Total training examples, after filtering:\", len(for_training), end='\\n\\n')\n",
    "    return for_training,non_training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write to files\n",
    "\n",
    "Get this list of words:  \n",
    "\n",
    "Current: `[{title, comment, line#, free_transl, origline_w_digits_punct, words\":[\n",
    "                {word_txt, wpostag, \"morphemes\":[\n",
    "                    [morph, morpheme, gloss, mpos, morphemetype]` \n",
    "\n",
    "Old: `[{\"text_title\":title, \"text_comment\":comment, \"words\":[\n",
    "            {\"segnum\":line#, \"orig_word\":word, \"POS\":postag, \"morphemes\": [\n",
    "                            [morph, morpheme, gloss, mpos, morphemetype]\n",
    "                         ]}]` \n",
    "\n",
    "to files with one word per line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_alignment(a, b):\n",
    "    if len(a) != len(b):\n",
    "        raise ValueError(\"must be same number of morph(emes) and gloss in a word\")\n",
    "        \n",
    "def poslines(listofwords, listofPOStags):\n",
    "    \"Arranges POS by sentences for training\"\n",
    "    stringtags = '%%'.join(listofPOStags)\n",
    "    listofwords = [''.join(word.split()) for word in listofwords]\n",
    "    stringwords = '%%'.join(listofwords)\n",
    "    bysenttags = stringtags.split('@EOS@')\n",
    "    bysentwords = stringwords.split('EOS')\n",
    "    return [' '.join(sent.split('%%')).strip() for sent in bysentwords], [' '.join(sent.split('%%')).strip() for sent in bysenttags]\n",
    "        \n",
    "\n",
    "def dataFiles(extracted_words, training_task, outfilepath):\n",
    "    '''Writes two files: X and y (tokens and annotations; input and output)'''\n",
    "    \n",
    "    input_data = []\n",
    "    output_data = []\n",
    "    \n",
    "    for word in extracted_words: \n",
    "        # input string (X)\n",
    "        input_data.append(' '.join(word[TOKEN])) # insert space between chars\n",
    "        # output types (y)\n",
    "        wPOS_tag = word[POS]\n",
    "        canonical_morphemes = []\n",
    "        surface_morphemes = []\n",
    "        glosses = []\n",
    "        for morpheme in word[MORPHEMES]:\n",
    "            surface_morphemes.append(morpheme[0])\n",
    "            canonical_morphemes.append(morpheme[1])\n",
    "            glosses.append(morpheme[2])\n",
    "\n",
    "        # determines what will be written to output file\n",
    "        #TODO: _canSegGls & _canSeg must handle null morphemes for non-neural models (CRF)\n",
    "        if training_task == '_pos':\n",
    "            output_data.append(wPOS_tag)\n",
    "        elif training_task == '_gls':\n",
    "            output_data.append(' '.join(glosses))\n",
    "        elif training_task == '_canSeg':\n",
    "            output_data.append(' '.join(canonical_morphemes))\n",
    "        elif training_task == '_surSeg':\n",
    "            output_data.append(' '.join(surface_morphemes))\n",
    "        elif training_task == '_surSegGls':\n",
    "            check_alignment(surface_morphemes, glosses)\n",
    "            combined_seg_gls = [morph+'#'+glosses[i] for i,morph in enumerate(surface_morphemes)]\n",
    "            output_data.append(' '.join(combined_seg_gls))\n",
    "        elif training_task == '_canSeg':\n",
    "            output_data.append(' '.join(canonical_morphemes))\n",
    "        elif training_task == '_canSegGls':\n",
    "            check_alignment(canonical_morphemes, glosses)\n",
    "            combined_seg_gls = [morpheme+'#'+glosses[i] for i,morpheme in enumerate(canonical_morphemes)]\n",
    "            output_data.append(' '.join(combined_seg_gls))\n",
    "        else:\n",
    "            print(\"Output format not found.\")\n",
    "    \n",
    "    # prepare for sentence level POS tagging\n",
    "    if training_task == '_pos':\n",
    "        input_data, output_data = poslines(input_data, output_data)\n",
    "    \n",
    "    with open(outfilepath+training_task+'.input', 'w', encoding='utf8') as I:\n",
    "        I.write('\\n'.join(input_data)[:-1])\n",
    "    with open(outfilepath+training_task+'.output', 'w', encoding='utf8') as O:\n",
    "        O.write('\\n'.join(output_data)[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Run Code: Extract Surface Segmentation Data to Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "####### EXTRACT from flextext#######\n",
    "def main(tostorepath, dbfile, bysentence=False):\n",
    "    master_data = extract_flextext(dbfile)\n",
    "\n",
    "    # NOTE: FIRST EDIT filtering() function to suit your task!\n",
    "    training_words, unannotated_words = filtering(master_data, bysentence)\n",
    "    print(\"Training examples:\\n\", training_words[-5:])\n",
    "    print(\"Unannotated data:\\n\", unannotated_words[-5:])\n",
    "    \n",
    "    for task in DESIRED_TASKS:\n",
    "        # write all extracted tokens to _M file \n",
    "        dataFiles(training_words+unannotated_words, task, tostorepath+'_Master')\n",
    "        # write filtered out tokens to _U(nannotated) file\n",
    "        if unannotated_words:\n",
    "            dataFiles(unannotated_words, task, tostorepath+'_U')\n",
    "        # write unfiltered tokens to T(raining) file\n",
    "        dataFiles(training_words, task, tostorepath+'_T')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts of speech found in corpus: {'coordconn', 'advlizer', 'interj', 'imp', 'nprop', 'verbprt', 'DM', 'cardnum', 'proform', 'pro', '@@@', 'nvp', 'onom', 'NUM', 'mod', 'dem', 'inter', 'v', 'nomprt', 'adj', 'n', 'quant', 'PUNCT', 'adv', 'dir', 'post'}\n",
      "\n",
      "All Tokens: 17587\n",
      "Lexemes, ignoring punctuation and digits: 14099\n",
      "\n",
      "[{'token': 'keey', 'wPOS': 'n', 'morphemes': [['keey', 'keey', 'village', 'n', 'stem']]}, {'token': 'tah', 'wPOS': 'post', 'morphemes': [['tah', 'tah', 'at:AR', 'post', 'stem']]}, {'token': 'hihneeshyąą', 'wPOS': 'v', 'morphemes': [['hih-', 'h-', '3PL.S.', 'v:Any', 'prefix'], ['nee-', 'nee-', 'QUAL:DH.PFV:Ø.', 'Verb', 'prefix'], ['shyąą', 'shyąą', 'grow:PFV', 'v', 'stem']]}, {'token': 'jah', 'wPOS': 'adv', 'morphemes': [['jah', 'jah', 'here', 'adv', 'stem']]}, {'token': 'dineh', 'wPOS': '@@@', 'morphemes': [['dineh', 'dineh', 'person', 'n', 'stem']]}, {'token': 'huuniign', 'wPOS': 'v', 'morphemes': [['huu-', 'huu-', '3SG.S:QUAL:DH.PFV:Ø.', 'Verb', 'prefix'], ['niign', 'niign', 'grab:PFV:NOM', 'v', 'stem']]}, {'token': \"u'aat\", 'wPOS': 'n', 'morphemes': [['u-', 'u-', '3SG.PSR.', 'Noun', 'prefix'], [\"'aat\", \"'aat\", 'wife', 'n', 'stem']]}, {'token': \"ts'exeh\", 'wPOS': 'n', 'morphemes': [[\"ts'exeh\", \"ts'exeh\", 'woman', 'n', 'stem']]}, {'token': 'nadįhtįį', 'wPOS': '@@@', 'morphemes': [['nadįhtįį', 'nadįhtįį', '@@@', '@@@', 'txt']]}, {'token': '.', 'wPOS': 'PUNCT', 'morphemes': [['.', '.', '@@@', 'PUNCT', 'punct']]}]\n",
      "\n",
      "Total training examples, after filtering: 7689\n",
      "\n",
      "Training examples:\n",
      " [{'token': '.', 'wPOS': 'PUNCT', 'morphemes': [['.', '.', '@@@', 'PUNCT', 'punct']]}, {'token': 'EOS', 'wPOS': '@EOS@', 'morphemes': ['@EOS@', '@EOS@', '@EOS@', '@EOS@', '@EOS@']}, {'token': \"t'axoh\", 'wPOS': 'adv', 'morphemes': [[\"t'axoh\", \"t'axoh\", 'finally', 'adv', 'stem']]}, {'token': '!', 'wPOS': 'PUNCT', 'morphemes': [['!', '!', '@@@', 'PUNCT', 'punct']]}, {'token': 'EOS', 'wPOS': '@EOS@', 'morphemes': ['@EOS@', '@EOS@', '@EOS@', '@EOS@', '@EOS@']}]\n",
      "Unannotated data:\n",
      " [{'token': ',\"', 'wPOS': 'PUNCT', 'morphemes': [[',\"', ',\"', '@@@', 'PUNCT', 'punct']]}, {'token': '<', 'wPOS': 'PUNCT', 'morphemes': [['<', '<', '@@@', 'PUNCT', 'punct']]}, {'token': 'shihenih', 'wPOS': '@@@', 'morphemes': [['shi-', 'sh-', '1SG.O.', 'Verb', 'prefix'], ['he-', 'he-', '3PL.S:Ø.IPV:Ø.', 'Verb', 'prefix'], ['nih', 'nih', 'say:IPV', 'v', 'stem']]}, {'token': '.>', 'wPOS': 'PUNCT', 'morphemes': [['.>', '.>', '@@@', 'PUNCT', 'punct']]}, {'token': 'EOS', 'wPOS': '@EOS@', 'morphemes': ['@EOS@', '@EOS@', '@EOS@', '@EOS@', '@EOS@']}]\n"
     ]
    }
   ],
   "source": [
    "####### FOR FILTERING ####### \n",
    "# lezgi pos tags: {'ordnum', 'Vnf', 'num', 'indfpro', 'nprop', 'emph', 'Vocpart', 'proform', 'multipnum', 'prep', 'adv', 'post', 'ptcp', 'pers', 'verbprt', 'coordconn', 'adj', 'v', 'conn', 'poss', 'pro', 'prt', 'det', 'dem', 'interj', 'msd', 'subordconn', 'Vf', 'cardnum', 'n', 'interrog', 'recp'}\n",
    "# Alas pos tags: {'num', 'n', 'refl', 'Aux', 'vt', 'cop', 'clf', 'adv', 'prt', 'Adj', 'cardnum', 'vi', 'stc', 'existmrkr', 'quant', 'relpro', 'ordnum', 'vd', 'distrnum', 'adj', 'Prep', 'nprop', 'interj', 'Conj', 'dem', 'v', 'pro'}\n",
    "# Upper Tanana Pos tags: {'dem', 'advlizer', 'nvp', 'nprop', 'inter', 'proform', 'imp', 'coordconn', 'v', '@@@', 'nomprt', 'verbprt', 'adv', 'adj', 'NUM', 'n', 'interj', 'pro', 'onom', 'PUNCT', 'cardnum', 'quant', 'mod', 'DM', 'dir', 'post'}\n",
    "# To decipher tasks: \n",
    "### gls = glossing only, seg = segmentation \n",
    "### can = canonical (underlying) morphemes, surf = surface morphs \n",
    "### SegGls = segmentation+glossing, pos = (word) POS tagging\n",
    "### All possible tasks: ['_canSeg', '_surSeg', '_gls', '_canSegGls', '_surSegGls', '_pos']\n",
    "DESIRED_TASKS = ['_pos']\n",
    "LANGS = ['tau']\n",
    "\n",
    "####### FILE LOCATIONS ####### \n",
    "for lang in LANGS:\n",
    "    STORE = r\"../../Teaching/NLPWorkshop/Alberta/data/\"+lang+'/'+lang\n",
    "    TO_EXTRACT = r'./flextexts/'+lang+'-all_txts.flextext'\n",
    "    main(STORE, TO_EXTRACT, bysentence=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
